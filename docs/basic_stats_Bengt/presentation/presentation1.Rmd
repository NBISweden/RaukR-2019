---
title: "Basic Statistics part"
subtitle: "RaukR 2019 • Advanced R for Bioinformatics"
author: "<b>Bengt Sennblad</b>"
institute: NBIS, SciLifeLab
keywords: r, RaukR, markdown
output: 
  xaringan::moon_reader:
    encoding: 'UTF-8'
    self_contained: false
    chakra: 'assets/remark-latest.min.js'
    css: 'assets/presentation.css'
    lib_dir: libs
    nature:
      ratio: '4:3'
      highlightLanguage: r
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "RaukR 2019 • %current%/%total%"
---
exclude: true
count: false

```{r,echo=FALSE,child="assets/header-presentation.Rmd"}
```

<!-- ----------------- Only edit title & author above this ----------------- -->

```{r,echo=FALSE,message=FALSE,warning=FALSE}
# load the packages you need

#library(tidyverse)
#library(ggplot2)
```
---
name: contents

## Contents

* [Introduction](#intro)

---
name: disposition

##disposition -- the real basics

* models -> stat models
* Use of stat models (type example linear regression)
  + simulation of data
      - variation
      - distribution
  + Probabilities of outcome
      - What are probabilities
      - conditional on model
      - full prob, conditional probs, likelihood, post probs-- Bayes theorem?
          * Bayesian and frequentists (not XOR!)
* Statistical tests
  + parametric (linear regression?)
  + non-parametric (GSEA?)
 
  
---
name: brainstorm

Things that may or may not be treated (preferably in a separate session)
  - multivariate linear models
  - confounders?
  - GLM, LMM 
  - sensitivity specificity ROC
  - PCA etc?
  
Things that would be very handy to have an intuition about for the exercise:
  - Matrix algebra version of linear model (e.g., for simulation exercise)
  - sampling from a distribution (rnorm, but what happens etc)
      
    

---
name: models1
## Models

* What is a model?

--
  + simplification (abstraction) of reality that helps us address a specific problem
  
---
## Models | `Example: R proficiency as a function of beer`
.pull-left-50[
We want to model how beer consumption affects the ability to program well in R

* $x$ number of beers drunk
* $y$ Ability to program in R
]

--
.pull-right-50[
```{r, beer1, echo=F, fig.height = 4.5}
x = seq(0,10)
b0 = 1.0
b1=-0.1
y = sapply(x, function(x) x*b1 + b0)
plot(x,y, type='b', ylab = "y = R proficiency", xlab = "x= Beers drunk")
```

]

--
This is a *linear model*, $$y = kx+c,$$ where $y$ changes in proportion to $x$.

***

--
__Notice that a model is always restricted by its assumptions:__
* In the model plot above, I have used strong assumptions about $k$ and $c$ being exactly $k=-0.1$ and $c=1.0$. 
* Therefore, people with a different susceptibility to alcohol $(k \neq -0.1)$ or a different initial proficiency in R $(c \neq 1.0)$ are *not* correctly modeled by this model.
  

---
## Models | `Example: R proficiency as a function of beer`
.center[ .large[How can we solve this? ] ]
Several possibilities (and we will discuss other common ones later), but one way is:

.pull-left-50[

* Create a *new* model taking this into account, where we let
  + we assume higher body mass gives less alcohol susceptibility (lower $k$)
  + for simplicity, assume also that lower body mass have lower initial proficiency (lower $c$).

* This will be a "hierarchical model" with:
  + a main linear model $$y=kx+c,$$ whose parameters are modeled by
  + linear submodels $k=f(mass)$ and $c=f(mass)$
]

--

.pull-right-50[
```{r beer2, echo=F, fig.height=5}
x= seq(0,10)
mass = c(90,80,70,60,50)
b1 = -0.55 + mass * 0.005
b0 = 0.1 + mass * 0.01
lin=function(x,k,c){
  return(k * x + c)
}
y = sapply(seq(1,5),function(i) lin(x, b1[i], b0[i]))
plot(x,y[,1], type='b', ylim = c(min(y), max(1,0,max(y))), 
     ylab = "y = R proficiency", xlab = "x= Beers drunk")
for(i in seq(2,5)){
  points(x,y[,i], type='b',col=i)
  legend("bottomleft", legend = paste(mass, " kg"), fill=seq(1,5))
  abline(h=0.0)
}
```
]
--

Still not quite right...


---

## Models | `Example`
.pull-left-50[  
More realistic?
```{r, beer3, echo=F, fig.height = 5}
x = seq(0,10)
b0 = 5
b1=-3

invlogit<-function(x, k, c){
  return(1/(1+exp(-k*(x-c))))
}
y = invlogit(x, b1, b0)
plot(x,y, type='b', 
     ylab = "y = R proficiency", xlab = "x= Beers drunk")
```

This is a logistic model $y = \frac{1}{1-e^{-kx+c}} \Leftrightarrow \log\frac{y}{1+y} = kx+c$, where there are saturation towards the extremes
]

--
.pull-right-50[  
* Models can also be described as a graph ('graph models'):
  + This is a graph model showing whether it is possible to fly to RaukR from a selection of cities of the world

```{r models1, echo=F, fig.width=5}
require(igraph)
towns = c("RaukR/Visby", "Stockholm", "Oslo", "Helsinki","Copenhagen", "Berlin", "Rome", "Moscow", "New York","Toronto", "Rio de Janeiro", "Stånga")
adj.matrix= matrix(rep(0, length(towns)*length(towns)), nr=length(towns), dimnames=list(towns, towns))
edges=list(c("RaukR/Visby", "Stockholm"), 
           c("Stockholm", "Oslo"), c("Stockholm", "Helsinki"), c("Stockholm","Copenhagen"), c("Stockholm","Berlin"), c("Stockholm","Berlin"),c("Stockholm","Rome"),c("Stockholm", "Moscow"), c("Stockholm","New York"),
           c("Oslo", "Helsinki"), c("Oslo","Copenhagen"), c("Oslo","Berlin"), c("Oslo","Rome"), c("Oslo","Moscow"), c("Oslo","New York"),
           c("Helsinki", "Copenhagen"), c("Helsinki","Berlin"), c("Helsinki","Rome"), c("Helsinki","Moscow"), c("Helsinki","New York"),
           c("Copenhagen", "Berlin"), c("Copenhagen","Rome"), c("Copenhagen","Moscow"), c("Copenhagen","New York"),
           c("Berlin", "Rome"), c("Berlin", "Moscow"), c("Berlin", "New York"), 
           c("Rome", "Moscow"), c("Rome", "New York"), c("Rome", "Rio de Janeiro"), 
           c("Moscow", "New York"), 
           c("New York","Toronto"), c("New York","Rio de Janeiro")
)

for(i in edges){
  adj.matrix[i[1],i[2]]=1
  adj.matrix[i[2],i[1]]=1
}
g <- graph.adjacency(adj.matrix, mode="undirected")
# How to set coordinates:
# t=tkplot(g)
# Manually adjust
# paste(tk_coords(t), collapse=",")
# Copy and paste coordinate vector below
coords = matrix(c(383,274,221,240,147,112,319,75,332,411,389,420,352,256,135,0,248,43,74,147,178,230,138,394), nrow=12)
#coords = matrix(c(438,313,233,240,122,92,371,158,344,375,463,451,275,257,146,0,62,157,183,261,76,12,142,322), nrow=12)
plot(g, layout=coords)
```  


]

---

name: models3

* model structure vs. model parameters (also observation space!?)

---

# But what can models be used for?

Perform 
## Task | `Simulation`
## Task | `Probability of data`
## Task | `Statistical test`

---
name: Simulation1

## Task | `Simulation`

$y = \beta_1* x + \beta_0$ is a *generating* model -- it describes how to generate $y$ from observed $x$ and parameters $k,c$.

* Generate 100 samples of $x,y$ by
  - generate 100 genotypes $x$ from a uniform distribution on integers $0,1,2$ (tip: use `runif`)
  - set $\beta_0=0.3$ and $\beta_0=0.3$
  - generate $y$ using the  model $y = \sim \beta_1* x + \beta_0$
  - plot $y$ against $x$.

.pull-left-50[
```{r, echo =TRUE, eval=FALSE}
#parameters
b0 = 0.3
b1 = 0.2
N=100
sim1 = data.frame(genos=round(runif(N,min=0,max=2)))

sim1$phenos = b0 + b1 * sim1$genos

plot(x=sim1$genos, y=sim1$phenos)
```
]

.pull-right-50[
```{r, echo =FALSE}
#parameters
b0 = 0.3
b1 = 0.2
N=100
sim1 = data.frame(genos=round(runif(N,min=0,max=2)))

sim1$phenos = b0 + b1 * sim1$genos
par(mfrow=c(2,1))

plot(x=sim1$genos, y=sim1$phenos)
```
]

---
name: Simulation2

## Task | `Simulation`

### Deterministic vs statistical models
* $y = \beta_1* x + \beta_0$ is a *deterministic* model
    + does not model any variation
    + Common, e.g., in classical physics ( $E=mc^3$ )
* $y = \beta_1* x + \beta_0 +\epsilon,$ where $\epsilon ~\sim N(0,\sigma^2)$ is a *statistical* (equiv. *stochastic*, *random*) model
    + attempts to model variation around a population mean (the *residuals*) determined by the model
    + used in statistic analysis

.pull-left-50[
```{r, echo =TRUE, eval=FALSE}
#parameters
b0 = 0.3
b1 = 0.2
N=100
sim1 = data.frame(genos=round(runif(N,min=0,max=2)))

sim1$phenos = b0 + b1 * sim1$genos + rnorm(N, mean=0, sd=0.05) 

plot(x=sim1$genos, y=sim1$phenos)
```
]

.pull-right-50[
```{r, echo =FALSE}
#parameters
b0 = 0.3
b1 = 0.2
N=100
sim1 = data.frame(genos=round(runif(N,min=0,max=2)))

sim1$phenos = b0 + b1 * sim1$genos + rnorm(N, mean=0, sd=0.05) 

par(mfrow=c(2,1))
plot(x=sim1$genos, y=sim1$phenos)
```
]

---
name: Probability1

## Task | `Probability of observed data`

For the linear model $$Y=\beta_0+\beta_1 X,$$ with parameters $(\beta_0=0.3, \beta_1=0.2, \sigma^2=0.05$), estimate $$Pr[Y <= 0.6|x=2].$$ 

### Simulation
* simulate 1000 $y$ using the model above with $x=2$ and store in a vector $Y$.
* Make a histogram of $Y$ and estimate (*tip*: make sure that $0.7$ is among to a histogram breakpoints)

.pull-left-50[
```{r, echo=TRUE, eval=FALSE}
#parameters
b0 = 0.3
b1 = 0.2
N = 1000
x = 2

y = b0 + b1 * x + rnorm(N, mean=0, sd=0.05) 

h=hist(y, frequency=TRUE, right=FALSE, breaks=seq(0,1.0, 0.1), ylim =c(0,1000), labels=TRUE)

# compute requested probability from hist
paste("Pr[Y<=0.6|X=2] = ",sum(h$counts[1:6])/N)
```
]
.pull-right-50[
```{r, echo=FALSE}
#parameters
b0 = 0.3
b1 = 0.2
N = 1000
x = 2

y = b0 + b1 * x + rnorm(N, mean=0, sd=0.05) 

par(mfrow=c(2,1))
h=hist(y, frequency=TRUE, right=FALSE, breaks=seq(0,1.0, 0.1), ylim =c(0,1000), labels=TRUE)

# compute requested probability from hist
paste("Pr[Y<=0.6|X=2] = ",sum(h$counts[1:6])/N)
```
]

---
name: Probability2

## Task | `Probability of observed data`

### Analytic solution | `challenge`
* Translate the requested interval of $y$ values to an interval of residuals
* Calculate the probability of the residual interval using the probability distribution for the Normal distribution (tip: look up help for `Normal`)
    + The distribution function of the vale $y$ is the probability that the stochastic variable $Y<=y$. *(Memo: introduce stochastic variable better and we need som plot explaining the probaility distribution)*

```{r, echo=TRUE}
b0 = 0.3
b1 = 0.2
y = 0.6

res = y - (b0 + b1 * x) 
paste("residuals = ",res)

paste("Pr[Y<=.6|X=2] = ",pnorm(res, sd=0.05))
```

* *Challenge*: try both approaches with other values of $y$ or try to compute $Pr[Y>y|X=2]$.
---
name: tests
## Task | `Statistical tests`

We will now consider an extension of the previous task to several data points. 

As you can imagine, doing this with the above approach could be cumbersome. 

Instead specific tests have been developed. 

### Student's t-test
Uses normalized residuals as a test statistics:

\begin{eqnarray*}
t  
&= \sum_{i}\frac{x_i-\mu}{s/\sqrt{N}} 
&= \frac{\bar{x}-\mu}{s/\sqrt{N}}
\end{eqnarray*}
where $s$ is the estimated standard deviation from the observed data. 

As you can see this boils down to comparing the (standardized) estimated mean from the data, $\bar{x}$ and the model mean, $\mu$. (*Challenge*: show the equality above)

Conveniently, The *t*-test is implemented in the R function `t-test`
---
name: tests
## Task | `Statistical tests`

* For simplicity, we will limit ourselves to the case $x=2$ (else *t*-test does not work -- handle this better)
* Perform a one sample t-test using $\mu$ from our original linear model, above, on new data simulated from a linear model with different parameters.
* *Challenge*: Try to use other parameters when simulating the new data.

.pull-left-50[
```{r, echo=TRUE, eval=FALSE}
# compute the mean, mu, from our model 
b0 = 0.3
b1 = 0.2
x = 2
mu = b0 + b1 * x

# Let's simulate the data from another model
b0 = 0.1
b1 = 0.5
N=100
y = b0 + b1 * rep(x,100) + rnorm(N, mean=0, sd=0.05) 
# t-test
t.test(y, mu=mu)
```
]


.pull-right-50[
```{r, echo=FALSE}
# compute mu from our model 
b0 = 0.3
b1 = 0.2
x = 2
mu = b0 + b1 * x

# Let's simulate the data from another model
b0 = 0.1
b1 = 0.5
N=100
y = b0 + b1 * rep(x,100) + rnorm(N, mean=0, sd=0.05) 

# t-test
t.test(y, mu=mu)
```
<br>
<br>
]     

.pull-left-50[

* In `lm`, a *t*-test is used to test if the estimated parameters are significantly different from 0. (mention NULL hypothse earlier)

```{r, echo=TRUE, eval=FALSE}

summary(lm(phenos~genos, data=sim1))
```
]
.pull-right-50[
```{r, echo=FALSE}
summary(lm(phenos~genos, data=sim1))
```
]

---
name: nonparam
## Parametric vs Non-parametric tests

The test we have used above are parametric, i.e., they are based on a parameterized model, e.g., the *t*-test assumes an underlying Normal distribution.

There are also non-parametric tests.

* definition of non-parametric tests a bit unclear
  - assumes model structure, but not params, or
  - assumes no model structure and no parameters
* non-parametric tests
  - often based on ranks of values are
  - often tests if two samples are simlar or significantly different

---
name: report

## Session  

* This presentation was created in RStudio using [`remarkjs`](https://github.com/gnab/remark) framework through R package [`xaringan`](https://github.com/yihui/xaringan).
* For R Markdown, see <http://rmarkdown.rstudio.com>
* For R Markdown presentations, see <https://rmarkdown.rstudio.com/lesson-11.html>

```{r,echo=TRUE}
R.version
```

---
name: end-slide
class: end-slide

# Thank you. Questions?
<!-- --------------------- Do not edit this and below --------------------- -->

---
name: end-slide
class: end-slide, middle
count: false

# Thank you. Questions?

```{r,echo=FALSE,child="assets/footer-presentation.Rmd"}
```

```{r,include=FALSE,eval=FALSE}
# manually run this to render this document to HTML
rmarkdown::render("presentation.Rmd")
# manually run this to convert HTML to PDF
#pagedown::chrome_print("presentation.html",output="presentation.pdf")
```

