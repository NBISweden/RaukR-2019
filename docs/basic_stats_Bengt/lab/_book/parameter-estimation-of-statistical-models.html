<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Parameter estimation of statistical models | Lab: Basic Statistics | Models</title>
  <meta name="description" content="4 Parameter estimation of statistical models | Lab: Basic Statistics | Models" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Parameter estimation of statistical models | Lab: Basic Statistics | Models" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Parameter estimation of statistical models | Lab: Basic Statistics | Models" />
  
  
  

<meta name="author" content="Bengt Sennblad" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="session-info.html">
<link rel="next" href="overfitting.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<link id="font-awesome-1-attachment" rel="attachment" href="libs/font-awesome-5.1.0/fonts/fontawesome-webfont.ttf"/>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="tasks.html"><a href="tasks.html"><i class="fa fa-check"></i><b>1</b> Tasks</a><ul>
<li class="chapter" data-level="1.1" data-path="tasks.html"><a href="tasks.html#task-simulation"><i class="fa fa-check"></i><b>1.1</b> Task | <code>Simulation</code></a><ul>
<li class="chapter" data-level="1.1.1" data-path="tasks.html"><a href="tasks.html#deterministic-model"><i class="fa fa-check"></i><b>1.1.1</b> Deterministic model</a></li>
<li class="chapter" data-level="1.1.2" data-path="tasks.html"><a href="tasks.html#statistical-model"><i class="fa fa-check"></i><b>1.1.2</b> Statistical model</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="tasks.html"><a href="tasks.html#task-probability-of-observed-data"><i class="fa fa-check"></i><b>1.2</b> Task | <code>Probability of observed data</code></a><ul>
<li class="chapter" data-level="1.2.1" data-path="tasks.html"><a href="tasks.html#simulation-solution"><i class="fa fa-check"></i><b>1.2.1</b> Simulation solution</a></li>
<li class="chapter" data-level="1.2.2" data-path="tasks.html"><a href="tasks.html#analytical-solution"><i class="fa fa-check"></i><b>1.2.2</b> Analytical solution</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="tasks.html"><a href="tasks.html#task-statistical-tests"><i class="fa fa-check"></i><b>1.3</b> Task | <code>Statistical tests</code></a><ul>
<li class="chapter" data-level="1.3.1" data-path="tasks.html"><a href="tasks.html#students-t-test"><i class="fa fa-check"></i><b>1.3.1</b> Student’s t-test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>2</b> Final words</a><ul>
<li class="chapter" data-level="2.1" data-path="final-words.html"><a href="final-words.html#types-of-models"><i class="fa fa-check"></i><b>2.1</b> Types of models</a></li>
<li class="chapter" data-level="2.2" data-path="final-words.html"><a href="final-words.html#types-of-statistical-tests"><i class="fa fa-check"></i><b>2.2</b> Types of statistical tests</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="session-info.html"><a href="session-info.html"><i class="fa fa-check"></i><b>3</b> Session info</a></li>
<li class="chapter" data-level="4" data-path="parameter-estimation-of-statistical-models.html"><a href="parameter-estimation-of-statistical-models.html"><i class="fa fa-check"></i><b>4</b> Parameter estimation of statistical models</a><ul>
<li class="chapter" data-level="4.1" data-path="parameter-estimation-of-statistical-models.html"><a href="parameter-estimation-of-statistical-models.html#bayesian-approach"><i class="fa fa-check"></i><b>4.1</b> Bayesian approach</a></li>
<li class="chapter" data-level="4.2" data-path="parameter-estimation-of-statistical-models.html"><a href="parameter-estimation-of-statistical-models.html#likelihood"><i class="fa fa-check"></i><b>4.2</b> Likelihood – The frequentist approach</a></li>
<li class="chapter" data-level="4.3" data-path="parameter-estimation-of-statistical-models.html"><a href="parameter-estimation-of-statistical-models.html#bayesians-vs-frequentists"><i class="fa fa-check"></i><b>4.3</b> Bayesians vs frequentists</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>5</b> Overfitting</a><ul>
<li class="chapter" data-level="5.1" data-path="overfitting.html"><a href="overfitting.html#overfitting-example-data"><i class="fa fa-check"></i><b>5.1</b> Overfitting | <code>Example data</code></a><ul>
<li class="chapter" data-level="5.1.1" data-path="overfitting.html"><a href="overfitting.html#task-simulation-of-example-data"><i class="fa fa-check"></i><b>5.1.1</b> Task | <code>simulation of example data</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="overfitting.html"><a href="overfitting.html#overfitting-model-comparison"><i class="fa fa-check"></i><b>5.2</b> Overfitting | <code>Model comparison</code></a><ul>
<li class="chapter" data-level="5.2.1" data-path="overfitting.html"><a href="overfitting.html#task-plot-two-likelihoods"><i class="fa fa-check"></i><b>5.2.1</b> Task | <code>plot two likelihoods</code></a></li>
<li class="chapter" data-level="5.2.2" data-path="overfitting.html"><a href="overfitting.html#task-plot-all-likelihoods"><i class="fa fa-check"></i><b>5.2.2</b> Task | <code>plot all likelihoods</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>6</b> Regularization</a><ul>
<li class="chapter" data-level="6.1" data-path="regularization.html"><a href="regularization.html#regularization-bayesian-interpretation"><i class="fa fa-check"></i><b>6.1</b> Regularization | <code>Bayesian interpretation</code></a></li>
<li class="chapter" data-level="6.2" data-path="regularization.html"><a href="regularization.html#regularization-aic-and-model-testing"><i class="fa fa-check"></i><b>6.2</b> Regularization | <code>AIC and model testing</code></a><ul>
<li class="chapter" data-level="6.2.1" data-path="regularization.html"><a href="regularization.html#task-aic-analysis"><i class="fa fa-check"></i><b>6.2.1</b> Task | <code>AIC analysis</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regularization.html"><a href="regularization.html#regularization-lasso-and-feature-selection"><i class="fa fa-check"></i><b>6.3</b> Regularization | <code>LASSO and Feature selection</code></a><ul>
<li class="chapter" data-level="6.3.1" data-path="regularization.html"><a href="regularization.html#task-lasso-using-the-glmnet-r-package"><i class="fa fa-check"></i><b>6.3.1</b> Task | <code>Lasso using the glmnet R-package</code></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="regularization.html"><a href="regularization.html#cross-validation"><i class="fa fa-check"></i><b>6.4</b> Cross-validation</a><ul>
<li class="chapter" data-level="6.4.1" data-path="regularization.html"><a href="regularization.html#cross-validation-how-to-test-for-overfitting"><i class="fa fa-check"></i><b>6.4.1</b> Cross-validation | <code>How to test for overfitting</code></a></li>
<li class="chapter" data-level="6.4.2" data-path="regularization.html"><a href="regularization.html#task-determine-optimal-lassolambdausing-cross-validation"><i class="fa fa-check"></i><b>6.4.2</b> Task | <code>Determine optimal LASSO</code><span class="math inline">\(\lambda\)</span><code>using cross-validation</code></a></li>
<li class="chapter" data-level="6.4.3" data-path="regularization.html"><a href="regularization.html#task-final-lasso-effect-sizes"><i class="fa fa-check"></i><b>6.4.3</b> Task| <code>Final LASSO effect sizes</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="session-info-1.html"><a href="session-info-1.html"><i class="fa fa-check"></i><b>7</b> Session info</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lab: Basic Statistics | <code>Models</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parameter-estimation-of-statistical-models" class="section level1">
<h1><span class="header-section-number">4</span> Parameter estimation of statistical models</h1>
<details>
<p><summary> Lecture notes </summary></p>
<p>Consider a generative model, with parameters <span class="math inline">\(\theta\)</span>, for how some data <span class="math inline">\(D\)</span>. We would like to test if <span class="math inline">\(\theta\)</span> are
good parameters or if some other parameters are better. Given the model, we can compute <span class="math display">\[Pr[D| \theta],\]</span> i.e., the probability that the model with parameters
<span class="math inline">\(\theta\)</span> generates <span class="math inline">\(D\)</span>.</p>
<p>However, we would be more interested in how good the model with parameters <span class="math inline">\(\theta\)</span> for our data. In other words, what we
would actually <em>like</em> to compute is <span class="math display">\[Pr[\theta|D].\]</span> This would allow us to select optimal parameter estimates and, importantly, to evaluate how good they are compared to other parameters.</p>
<p>Getting from <span class="math inline">\(Pr[D| \theta]\)</span> to <span class="math inline">\(Pr[\theta|D]\)</span> can be solved in different ways, which has given rise to two major
philosofical branches of statistics:</p>
<ul>
<li><em>Bayesian statistics</em> and</li>
<li><em>Frequentist statistics</em></li>
</ul>
</details>
<div id="bayesian-approach" class="section level2">
<h2><span class="header-section-number">4.1</span> Bayesian approach</h2>
<details>
<p><summary> Lecture notes </summary></p>
<p>Bayes’ theorem (Thomas Bayes, 1702-1761) provides a way to obtain the requested <span class="math inline">\(P[\theta|X,Y]\)</span></p>
<p><span class="math display">\[Pr[\theta|D] = \frac{Pr[D| \theta]Pr[\theta]}{Pr[D]}\]</span>
<strong>Posterior probability</strong></p>
<p><span class="math inline">\(Pr[\theta|D],\)</span> the probability, computed posterior to analysis, of the parameters <span class="math inline">\(\theta\)</span> conditioned on the observed data, i.e, our requested probability.</p>
<p>An important characteristic of Bayesian statistics is that the focus is not on point estimates, but on the posterior probability distribution over the parameter space of <span class="math inline">\(\theta\)</span>, which provides a measure of uncertainty (probabilities) in comparison to other values.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-15-1.svg" width="672" style="display: block; margin: auto auto auto 0;" /></p>
<p><strong>Prior probability of <span class="math inline">\(\theta\)</span></strong></p>
<p><span class="math inline">\(Pr[\theta]\)</span> is the <em>prior</em> probability of <span class="math inline">\(\theta\)</span> and should according to Bayesian statistics reflect what we know (or believe to know) about how close <span class="math inline">\(\theta\)</span> is to the true parameters. We can use information from previous studies or we can assign a <em>uninformative</em> prior, e.g., <span class="math inline">\(Pr[\theta]\)</span> follows a uniform distribution for all <span class="math inline">\(\theta\)</span> in the interval <span class="math inline">\([a,b]\)</span>.</p>
<p>It can be shown that the effect of the prior on the posterior probsbiity is largest when the observed data is small. With larger sample sizes, the posterior probability will eventually just depend on <span class="math inline">\(Pr[D|\theta]\)</span>.</p>
<p><strong>Marginal Probability of <span class="math inline">\(D\)</span></strong></p>
<p><span class="math inline">\(Pr[D]=\int_{\theta}Pr[D| \theta]Pr[\theta]\)</span> is the probability of <span class="math inline">\(D\)</span> regardless of <span class="math inline">\(\theta\)</span>. This can often be difficult difficult to calculate and, for this reason, Bayesian models are often designed so that this can be calculated analystically or some approximation approach, such as Markov chain Monte Carlo (MCMC) is used.</p>
<details>
<p><summary> Extra reading </summary></p>
<p><strong>Probabilistic algebra</strong></p>
<p>A conditional probability <span class="math inline">\(Pr[A|B]\)</span> is the probability that <span class="math inline">\(A\)</span> happens if we know that <span class="math inline">\(B\)</span> has happened.
To obtain the probability that both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> happens we need to first take the probability that <span class="math inline">\(B\)</span> happens and then multiply it with the conditional probability that <span class="math inline">\(A\)</span> hapens given <span class="math inline">\(B\)</span>, i.e.,:</p>
<p><span class="math display">\[Pr[A,B] = Pr[A|B] Pr[B].\]</span></p>
<p>From this follows the reverse operation</p>
<p><span class="math display">\[\frac{Pr[A,B]}{Pr[B]} = Pr{A|B}\]</span>
Notice that this also works if we have more than one condition:
<span class="math inline">\(Pr[A|B,C] * Pr[B] = Pr[A,B|C].\)</span></p>
<p>What happens in Bayes rule is that we first, in the numerator, perform <span class="math inline">\(Pr[B|A]*Pr[A] = Pr[A,B]\)</span> and then divide this with the denominator <span class="math inline">\(\frac{Pr[A,B]}{Pr[B]} = Pr[A|B]\)</span>.</p>
<hr />
</details>
<hr />
</details>
</div>
<div id="likelihood" class="section level2">
<h2><span class="header-section-number">4.2</span> Likelihood – The frequentist approach</h2>
<details>
<p><summary> Lecture notes </summary></p>
<p>Likelihood (Introduced by Fisher, 1925, formalized by Edwards, 1972) builds on the intuition that if <span class="math inline">\(\theta\)</span> is close to the ‘truth’, then <span class="math inline">\(Pr[Y| X, \theta]\)</span> will be higher than for wrong <span class="math inline">\(\theta\)</span>. We should therefore select the <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(Pr[Y| X, \theta]\)</span>; this is called maximum likelihood estimation (MLE) of <span class="math inline">\(\theta\)</span>.</p>
<p>Since statistical model contain an element of randomness, the reasoning above might not always be correct for any single obeservation. However, if we sum over a large number of observations it will be true on average. Hence the need for datasets that are large enough.</p>
<p>To formalize this intuition, Edwards (1972) defined the likelihood of model parameters being true given observed data as</p>
<p><span class="math display">\[L[\theta|D] \propto Pr[D| \theta]\]</span></p>
<details>
<p><summary> Extra Reading </summary></p>
<p>Notice that this notation is not uncommonly mixed up, so you might also see the notation <span class="math inline">\(L[Y|X,\theta]\)</span> for the likelihood.</p>
<p>Similarly <span class="math inline">\(\propto Pr[Y|X, \theta]\)</span> is often referred to as the <em>likelihood function</em>.</p>
<hr />
</details>
<p>The proportionality (indicated by ‘<span class="math inline">\(\propto\)</span>’) means there are some unknown constant factor, <span class="math inline">\(k\)</span>, such that <span class="math inline">\(L[\theta|Y,X] = k Pr[Y|X, \theta]\)</span>. However, the factor <span class="math inline">\(k\)</span> is assumed to be constant over <span class="math inline">\(\theta\)</span>s and over models.</p>
<p>Using a Bayesian perspective, we can see that the proportionality constant <span class="math inline">\(k = \frac{Pr[\theta]}{Pr[D]}\)</span>, and that Likelihood would correspond to assuming a uniform prior over all possible values of <span class="math inline">\(\theta\)</span>.</p>
<p>In practice, the proportionality is ignored and we set</p>
<p><span class="math display">\[L[\theta|Y,X] = Pr[Y|X, \theta]\]</span></p>
<details>
<p><summary> Extra Reading </summary></p>
<p>When the likelihood of two <span class="math inline">\(\theta\)</span>s (or models) are compared this is almost always done as a <em>likelihood ratio</em>,</p>
<p><span class="math display">\[\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]} = \frac{k Pr[Y|X, \theta_1]}{ k  Pr[Y|X, \theta_0]} =\frac{Pr[Y|X, \theta_1]}{ Pr[Y|X, \theta_0]}\]</span></p>
<p>which means that the factor <span class="math inline">\(k\)</span> disappears. Hence the factor <span class="math inline">\(k\)</span> is always ignored. Likelihood ratios is the basis of most model comparison statistics, e.g., the Wald test, the Score test, regularization…</p>
<hr />
</details>
<p>In maximum likelihood estimation of some parameters <span class="math inline">\(\theta\)</span>, one simply selects the estimates <span class="math inline">\(\widehat\theta\)</span> that gives the highest likelihood, <span class="math inline">\(max_{\theta}L[\theta|X,Y] = L[\widehat\theta|X,Y]\)</span>. In many applications of likelihood and maximum likelihood, it is practical to instead use the logarithm of the likelihood, the logLikelihood, <span class="math inline">\(\log L[\theta_1|Y,X]\)</span>.</p>
<details>
<p><summary> Extra Reading </summary></p>
<p>As mentioned above, the logarithm of the likelihood, the logLikelihood, <span class="math inline">\(\log L[\theta_1|Y,X]\)</span>, or sometimes the negative logLikelihood, <span class="math inline">\(-\log L[\theta_1|Y,X]\)</span>, is often used. Notice, that</p>
<ol style="list-style-type: decimal">
<li>The <span class="math inline">\(\theta\)</span> estimates that maximizes <span class="math inline">\(\log L[\theta|Y,X]\)</span> also maximizes <span class="math inline">\(L[\theta|Y,X]\)</span></li>
<li>The <span class="math inline">\(\theta\)</span> estimates that minimizes <span class="math inline">\(-\log L[\theta|Y,X]\)</span> maximizes <span class="math inline">\(L[\theta|Y,X]\)</span></li>
<li>A likelihood ratio corresponds to a logLikelihood difference, <span class="math display">\[\log\left(\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]}\right) = \frac{\log L[\theta_1|Y,X]}{\log L[\theta_0|Y,X]} = \log L[\theta_1|Y,X] - \log L[\theta_0|Y,X]\]</span>.</li>
</ol>
<hr />
</details>
<p>Likelihood and maximum likelihood estimation are central concepts in frequentist statistics. Many statistical tests and methods uses or is based on the concept of maximum likelihood.</p>
<p>In general, full-on likelihood computation and maximum likelihood estimation is relatively slow, so alternative and faster methods has been developed. One example is the use <em>ordinary least squares</em> OLS for linear models; it can be shown that the likelihood can be expressed as a function of the <em>residual sum of squares</em> (RSS) and that maximum likelihood estimates of <span class="math inline">\(\beta\)</span> is exactly the same as those of the OLS (which minimizes RSS.</p>
<p><em><strong>NB!</strong> This is a special case for linear models and are not generally true for other models. For example, logistic regression is typically fitted using maximizing the likelihood </em></p>
<details>
<p><summary> Extra Reading </summary></p>
<p>Linear models is a special case with some nice properties when it comes to likelihood. Consider a simple linear regression model,</p>
<p><span class="math display">\[ y = \beta x + \epsilon, \]</span></p>
<p>where the residuals <span class="math inline">\(\epsilon\sim N(0,\sigma^2)\)</span>.</p>
<p>It turns out that the likelihood estimates of both <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> are functions of the RSS of the residuals, so that the likelihood can be approximated by</p>
<p><span class="math display">\[  \log L[\beta, \sigma^2|Y,X] \approx -\frac{N}{2} \log RSS\]</span></p>
<p>The likelihood for given <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>, given observed data <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[ L[\beta, \sigma^2|Y,X] = \prod_i pdf_{Normal}(y_i, \mu=\beta x_i, \sigma^2=\sigma^2) = \prod_i \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(y_i-\beta x_i)^2}{2\sigma^&quot;}} \]</span></p>
<p>where <span class="math inline">\(pdf_{Normal}\)</span> denotes the probability distribution function for the Normal distribution. If we work with the logLIkelihood instead, we get</p>
<p><span class="math display">\[\begin{eqnarray*}
\log L[\beta, \sigma^2|Y,X] 
&amp;=&amp; \sum_{i=1}^N \log\left(\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(y_i-\beta x_i)^2}{2\sigma^2}}\right)\\
&amp;=&amp;   \sum_{i=1}^N \log \left(\frac{1}{\sqrt{2\pi \sigma^2}}\right) -\frac{(y_i-\beta x_i)^2}{2\sigma^2} \\
&amp;=&amp;   N\log \left(2\pi \sigma^2\right)^{-1/2} -\frac{\sum_{i=1}^N (y_i-\beta x_i)^2}{2\sigma^2} \\
&amp;=&amp;   -\frac{N}{2}\log \left(2\pi \sigma^2\right)  -\frac{RSS}{2\sigma^2}
\end{eqnarray*}\]</span></p>
<p>We see here that minimizing <span class="math inline">\(RSS\)</span> (as in OLS) will maximize the logLikelihood, regardless of the value of <span class="math inline">\(\sigma^2\)</span>. Moreover, it turns out that also <span class="math inline">\(\sigma^2\)</span> can be estimated fairly well by <span class="math inline">\(RSS/N\)</span>. Hence, we get</p>
<p><span class="math display">\[\begin{eqnarray*}
\log L[\beta, \sigma^2|Y,X]
&amp;=&amp;   -\frac{N}{2}\log \left(\frac{2\pi RSS}{N}\right)  -\frac{N}{2}\frac{RSS}{RSS}\\
&amp;=&amp;   -\frac{N}{2}\log RSS + \frac{N}{2}\log \frac{2\pi}{N} -\frac{N}{2}\\
&amp;=&amp;   -\frac{N}{2}\log RSS + C
\end{eqnarray*}\]</span>
where <span class="math inline">\(C=\frac{N}{2}\left(\log \frac{2\pi}{N} -1\right)\)</span> is a constant that is usually ignored (in likelihood ratios, which is equivalent to log likelihoods differences, it will disappear).</p>
<hr />
</details>
<hr />
</details>
</div>
<div id="bayesians-vs-frequentists" class="section level2">
<h2><span class="header-section-number">4.3</span> Bayesians vs frequentists</h2>
<details>
<p><summary> Lecture notes </summary></p>
<p>There is often described a severe controversy between Bayesians and frequentists. However, this controversy represents the extreme hardcore Bayesians and frequentists.</p>
<p>In reality, there is a large gray-zone where frequentists and Bayesians meet and socialize:</p>
<ul>
<li>Bayesian models can be viewed as a type of the hierarchical models often used by frequentists</li>
<li>Frequentist bootstrap analysis is often used to estimate uncertainty of point estimates in relation to alternatives, as is done in Bayesian statistics</li>
<li>The <em>Bayes factor</em> is a Bayesian version of the likelihood ratio</li>
<li>Bayesian <em>posterior intervals</em> corresponds to frequentist <em>confidence intervals</em> (<em>Note</em> however, that there are no Bayesian significance test)</li>
<li>etc.</li>
</ul>
<p>Most practical statisticians use the tool that is adequate for the problem at hand, whether it is Bayesian or frequentist.</p>
<hr />
</details>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="session-info.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="overfitting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
