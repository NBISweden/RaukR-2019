[
["tasks.html", "Lab: Basic Statistics | Models RaukR 2019 • Advanced R for Bioinformatics 1 Tasks 1.1 Task | Simulation 1.2 Task | Probability of observed data 1.3 Task | Statistical tests", " Lab: Basic Statistics | Models RaukR 2019 • Advanced R for Bioinformatics Bengt Sennblad 1 Tasks 1.1 Task | Simulation Here, we will generate a synthetic data set by simulating data from a linear model. \\(Y = \\beta_0 + \\beta_1* X\\) is a generating model – it describes how to generate \\(Y\\) from observed \\(X\\) and parameters \\(\\beta_0,\\beta_1\\). 1.1.1 Deterministic model Generate 100 samples of \\(x,y\\) by generate genotypes \\(x\\) for each sample from a uniform distribution on integers \\(0,1,2\\) (tip: use runif) set \\(\\beta_0=0.3\\) and \\(\\beta_1=0.2\\) generate a phenotype \\(y\\) for each sample of using the model \\(y = \\beta_0 + \\beta_1* x\\) plot \\(y\\) against \\(x\\). 1.1.1.1 Think about Does the plotted results look biologically reasonable? if not: what could be the reason? #parameters b0 = 0.3 b1 = 0.2 N=100 sim1 = data.frame(x=round(runif(N,min=0,max=2))) sim1$y = b0 + b1 * sim1$x plot(x=sim1$x, y=sim1$y) Some possible answers Does the plot look biologically reasonable? If you followed the formula \\(y = \\beta_0 + \\beta_1* x\\) in the instructions literally, you end up with something like this This means that you have effectively demonstrated what a deterministic model is and that is a good thing pedagoically :D Notice that there are no variation in \\(y\\) for each genotyp \\(X\\), this is not what we see in biological data. Go on and read more below under Statistical model 1.1.2 Statistical model Deterministic vs statistical models \\(Y = \\beta_0 + \\beta_1* X\\) is a deterministic model does not model any variation Common, e.g., in classical physics (velocity \\(v = \\frac{\\Delta x}{\\Delta t}\\)) \\(Y = \\beta_0 + \\beta_1* X +\\epsilon,\\) where \\(\\epsilon~\\sim N(mean =0,sd=\\sigma)\\) is a statistical (equiv. stochastic, random) linear model (\\(\\sim\\) means ‘is drawn from’) attempts to model variation around a population mean (the residuals) determined by the model the residual of sample \\(i\\) is \\(y_i-(\\beta_0 + \\beta_1* X)\\). used in statistic analysis \\(Y\\) is then called a stochastic or random variable Henceforth, we will only consider statistical models. Task Generate data from \\(Y = \\beta_0+ \\beta_1* X +\\epsilon,\\) where \\(\\epsilon ~\\sim N(mean=0,sd = 0.05)\\) (tip: use rnorm). #parameters b0 = 0.3 b1 = 0.2 N=100 sim1 = data.frame(x=round(runif(N,min=0,max=2))) sim1$y = b0 + b1 * sim1$x + rnorm(N, mean=0, sd=0.05) plot(x=sim1$x, y=sim1$y) 1.1.2.1 Think about Does the result look more biologically plausible? When can simulated data be useful? Some possible answers Biologically plausible Now we have some variation around the mean \\(Y\\) for each genotype \\(X\\). However, we still cannot be sure that the arbitrarily chosen \\(\\beta_0\\) and \\(\\beta_1\\) are correct and maybe there are more variables (or covariates), affecting \\(y\\), that are not in the model. Always think about what the assumptions of the model is and if they are reasonable! Uses for simulated data Oracle knowledge when evaluating performance of methods, e.g., Type I and II errors Estimating probabilities and probability distributions of, e.g., data and summary statistics of data (next task) 1.2 Task | Probability of observed data A (statistical) linear model, \\(Y=\\beta_0+\\beta_1 X,\\) generates \\(Y\\) given, or conditioned on, specified value of \\(X\\). Since we have variation in the model, we can’t say exactly what value \\(Y\\) will be. Instead, we can compute or estimate the probability that \\(Y\\) takes a specific value, say \\(Y=y\\), if we know that \\(X\\) has a specific value, say \\(X=x\\) (we say that the probaility is conditioned this value of \\(X\\)). This will be a conditional probability \\(Pr[Y=y|X=x]\\), where the bar (‘\\(|\\)’) means that the probability is conditioned on \\(X=x\\). In fact, if \\(X\\neq x\\) then this probability tells us nothing about \\(Y\\). Our task here is to, for the linear model \\[Y=\\beta_0+\\beta_1 X\\] with parameters \\((\\beta_0=0.3, \\beta_1=0.2, \\sigma=0.05\\)), estimate the conditional probability \\[Pr[Y &lt;=0.65|X=2].\\] 1.2.1 Simulation solution simulate 1000 \\(y\\) using the model above with \\(x=2\\) and store in a vector \\(Y\\). Make a histogram of \\(Y\\) and estimate the probability that \\(Y&lt;=0.65\\) (tip: make sure that \\(0.65\\) is among to a histogram breakpoints) Think About What shape does the plotted histogram have? Where approximately is the mean? Does this make sense in light of the generative model we used? Can we plot \\(Pr[Y&lt;=y|X,\\theta]\\) more directly? (tip: google, e.g., ‘probability distribution’) #parameters b0 = 0.3 b1 = 0.2 N = 1000 x = 2 y = b0 + b1 * x + rnorm(N, mean=0, sd=0.05) h=hist(y, frequency=TRUE, right=FALSE, breaks=seq(0,1.0, 0.05), plot=FALSE) h$counts = h$counts/N plot(h, xlim=c(0,1.0), ylim =c(0,1), labels=TRUE) # compute requested probability from hist paste(&quot;Pr[Y&lt;=0.65|X=2] = &quot;, sum(h$counts[1:13])) Some possible answers Shape The plot should be more or less bell-shaped (how well this show might depend on how many breaks you had for the histogram) Mean The mean appers to be approx 0.7 Makes sense? A bell-shaped distribution is typical for the Normal distribution, which makes sense since the residuals are sampled from a normal distribution. However, in our simulation we used a mean = 0, and not 0.7 – how does this work? The reason is that (\\(\\Leftrightarrow\\) means ‘is equivalent to’) \\[N(mean = \\mu, sd =\\sigma) \\Leftrightarrow N(mean = 0, sd = \\sigma) + \\mu,\\] meaning that adding \\(\\mu\\) moves the whole distribution to a new mean \\(=\\mu\\). this means that our model can equivalently be written \\(Y = N(\\beta_0+\\beta_1*X, \\sigma)\\) What have we plotted; can we plot \\(Pr[Y&lt;=y|X,\\theta]\\) more directly? We have plotted the distribution of interval probabilities of \\(N(\\beta_0+\\beta_1*X, \\sigma)\\). An important distribution in statistics is the distribution of the cumulative distribution function (CDF), which is exactly the probability we estimated above, namely \\(Pr[Y &lt;= y| \\mu, \\sigma]\\) for a normal distribution \\(N(mean = \\mu, sd=\\sigma)\\). The code below plots the estimated CDF from our simulation; notice the typical sigmoid shape. From this distribution, we can directly read \\(Pr[Y&lt;=0.65|X,\\theta]\\) from the ppropriate bar. #parameters b0 = 0.3 b1 = 0.2 N = 1000 x = 2 y = b0 + b1 * x + rnorm(N, mean=0, sd=0.05) h=hist(y, frequency=TRUE, right=FALSE, breaks=seq(0,1.0, 0.05), plot=FALSE) h$counts = h$counts/N # replace the histogram counts with the cumulative counts h$counts = cumsum(h$counts) #plot the CDF plot(h, labels=TRUE) # compute requested probability from hist paste(&quot;Pr[Y&lt;=0.65|X=2] = &quot;, sum(h$counts[13])) ## [1] &quot;Pr[Y&lt;=0.65|X=2] = 0.145&quot; Extra reading The General Normal distribution Additionally, multiplying a normal distribution with \\(\\sigma\\) changes the ‘width’ of the distribution to have the satndard deviation \\(\\sigma\\), meaning that we have \\[N(mean = \\mu, sd =\\sigma) \\Leftrightarrow N(mean = 0, sd = 1)* \\sigma + \\mu, \\] something that is very much used in statistics The distribution \\(N(mean=0, sd=\\sigma)\\) is called the general normal distribution Any variable \\(Y\\sim N(mean= \\mu, sd=\\sigma)\\) can therefore be transformed into a Z-value, \\(Z=\\frac{Y-\\mu}{\\sigma},\\) which has a general normal distribution, i.e., \\(Z \\sim N(mean=0, sd=1)\\) 1.2.2 Analytical solution Conveniently, R has both the PDF and CDF implemented, making a Calculate \\(Pr[Y&lt;=0.65|X=2]\\) using R’s cumulative distribution function (CDF) for the Normal distribution (tip: look up help for Normal) The CDF of the value \\(y\\) is the probability that the stochastic variable \\(Y&lt;=y\\). Challenge: try both approaches with other values of \\(y\\) or try to compute \\(Pr[Y&gt;y|X=2]\\). Think about Do the result fit that from the Simulation? If not, can make the fit better? What can this result be used for? Challenge: Why do we look at intervals of \\(Y\\) rather than specific values? b0 = 0.3 b1 = 0.2 x = 2 y = 0.65 mu = b0 + b1 * x paste(&quot;Pr[Y&lt;=.6|X=2] = &quot;,pnorm(y, mean = mu, sd=0.05)) Some possible answers The result should fit quite well with that from the Simulation approach, given that precision obtained from the histogram The precision can be improved with a higher sample size for the simulation The CDF, i.e., \\(Pr[Y&lt;=y|X=2]\\), is actually the basis for p-values. A p-value for an observation \\(y\\) under a model is defined as the probability of generating \\(y\\) or more extreme values from the model. p-values can refer to different meanings of extreme. Using our model as an example NULL model: For a left-tailed p-value, \\(p(y) = Pr[Y&lt;=y|X=x]\\), i.e., the \\(CDF(y, \\mu, \\sigma)\\) For a right-tailed p-value, \\(p(y) = Pr[Y&gt;=y|X=x]\\), i.e., \\(1-CDF(y, \\mu, \\sigma)\\) For a double-tailed p-value, \\(p(y) = 2 \\min\\{Pr[Y&lt;=y|X=x],Pr[Y&gt;=y|X=x]\\}\\) If the p-value is significant, i.e., below some threshold \\(\\alpha\\) (typically \\(\\alpha = 0.05\\)) we can reject the NULL hypothesis that the data is generated from our model. What was the left-tailed p-value for \\(y=0.65\\) under our model and was that significant? What can we say about our model as a NULL model. Extra Reading Why probability of interval This is a bit tricky to explain. For discrete variable \\(Y\\), say heads or tails, we can compute an exact probability for, e.g. heads. However, for continuous \\(Y\\), it is not possible compute the probability of an exact value of \\(Y\\), since exact must be defined using an inifite number of decimals. Thus, in effect makes \\(Pr[Y] = 0\\)! Hence, only intervals of continuous variables can be assigned proper probabilities. However, it is, in statistics, very convenient to be able to talk about and work with exact values \\(Pr[Y=y]y\\) also for continuous variables. If we reduce the interval centered around \\(Y=y\\) decrementally and each time compute the mean probability over the histogram interval of intererest, we can get a better and better approximation of \\(Pr[Y=y]\\), we will in the limit end up with the Probability density of \\(Y=y\\). The distribution of probability densities for \\(Y\\) is called the Probability density function (PDF) of \\(Y\\) Mathematically, the PDF can be viewed as the derivative of the CDF Notice that the density of \\(Y\\) is not a probability (e.g., it can be higher than 1.0). Sometimes this is marked by using a different notation for the density, e.g., \\(f(Y)\\); here however, we will use \\(Pr[Y]\\) also for the density. 1.3 Task | Statistical tests We will now consider an extension of the previous task to several data points. As you can imagine, doing this with the above approach could be cumbersome. Instead specific tests have been developed. We will here use the t-test as a very simple example. 1.3.1 Student’s t-test We will use a single sample t-test, which tests how probable it is that some sampled data is drawn from a given Normal distribution; this distribution is called the NULL model of the test. The t-test is not designed to handle different genotypes, so for simplicity, we will, again, limit ourselves to the case \\(X=2\\). The t-test uses normalized residuals as a test statistics: \\[\\begin{eqnarray*} t &amp;= \\sum_{i}\\frac{y_i-\\mu}{s/\\sqrt{N}} &amp;= \\frac{\\bar{y}-\\mu}{s/\\sqrt{N}} \\end{eqnarray*}\\] where \\(s/\\sqrt{N}\\) is an estimate of the standard deviation, \\(\\sigma\\), from the observed data. As you can see this boils down to comparing the (standardized) estimated mean from the data, \\(\\bar{y}\\) and the model mean, \\(\\mu\\). Challenge: show the equality above` These normalized residuals are approximately distributed \\(t~\\sim N(0,1)\\) (this is the General Normal distribution, see also ‘Possible answers/Extra Reading’ under 1.2.1) Conveniently, The t-test is implemented in the R function t.test Simulate a new data set from a linear model \\(Y&#39; = \\gamma_0 + \\gamma_1 X&#39;\\), for \\(X&#39;=2\\) for all samples. For simplicity, we will limit ourselves to the case \\(X&#39;=2\\) Use our previous model \\(Y=\\beta_0+\\beta_1 X\\) as the NULL model. Perform a one sample t-test (with \\(\\mu\\) from the NULL model) on new \\(Y&#39;\\) data. Try some different values of \\((\\gamma_0, \\gamma_1)\\) closer or further away from \\((\\beta_0, \\beta_1)\\) Think about Does the t-test reject the NULL model or not? # compute the mean, mu, from our model b0 = 0.3 b1 = 0.2 mu = b0 + b1 * x # Let&#39;s simulate the data from another model c0 = 0.4 c1 = 0.2 N=100 yp = c0 + c1 * rep(x,N) + rnorm(N, mean=0, sd=0.05) # t-test t.test(yp, mu=mu) Some possible answers * The t-test appears quite sensitive, especially for changes of \\(\\gamma_1\\) (c_1) The t-test can also be applied to other questions Test if two samples are generated by the same (unknown) model using the standardized difference in means, typically, using the NULL hypothesis that this difference is 0. Example code below # data 1 b0 = 0.3 b1 = 0.2 N=100 y = b0 + b1 * rep(x,N) + rnorm(N, mean=0, sd=0.05) #data 2 c0 = 0.4 c1 = 0.2 N=100 yp = c0 + c1 * rep(x,N) + rnorm(N, mean=0, sd=0.05) t.test(y,yp) Test if estimated parameters in a linear model are significantly different from 0 (see code). Example code below # reuse previously simulated data in sim1 b0 = 0.3 b1 = 0.2 N=100 sim1 = data.frame(x=round(runif(N,min=0,max=2))) sim1$y = b0 + b1 * sim1$x summary(lm(y~x, data=sim1)) "],
["final-words.html", "2 Final words 2.1 Types of models 2.2 Types of statistical tests", " 2 Final words 2.1 Types of models Deterministic models structure parameter values Statistical models Additional random distribution Parameteric models structure fixed a priori Example: Normal family of distribution (incl. linear model) Non-parametric distribution estimates structure from data Example: histograms, Kernel density estimation 2.2 Types of statistical tests parametric tests assumes statistical distribution a priori Student’s t-test ANOVA non-parametric tests no assumption of statistical distribution typically uses ranking of values Example: Mann-Whitney U tests Kruskal-Wallis test "],
["session-info.html", "3 Session info", " 3 Session info ## R version 3.5.3 (2019-03-11) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] captioner_2.2.3 igraph_1.2.4.1 bookdown_0.11 knitr_1.23 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.1 rstudioapi_0.10 servr_0.13 magrittr_1.5 ## [5] R6_2.4.0 stringr_1.4.0 tools_3.5.3 packrat_0.5.0 ## [9] xfun_0.7 xaringan_0.10 htmltools_0.3.6 yaml_2.2.0 ## [13] digest_0.6.19 later_0.8.0 promises_1.0.1 rsconnect_0.8.13 ## [17] evaluate_0.14 mime_0.6 rmarkdown_1.13 stringi_1.4.3 ## [21] compiler_3.5.3 jsonlite_1.6 httpuv_1.5.1 pkgconfig_2.0.2 Built on: 14-Jun-2019 at 11:31:01. 2019 • SciLifeLab • NBIS • RaukR "],
["parameter-estimation-of-statistical-models.html", "4 Parameter estimation of statistical models 4.1 Bayesian approach 4.2 Likelihood – The frequentist approach 4.3 Bayesians vs frequentists", " 4 Parameter estimation of statistical models Lecture notes Consider a generative model, with parameters \\(\\theta\\), for how some data \\(D\\). We would like to test if \\(\\theta\\) are good parameters or if some other parameters are better. Given the model, we can compute \\[Pr[D| \\theta],\\] i.e., the probability that the model with parameters \\(\\theta\\) generates \\(D\\). However, we would be more interested in how good the model with parameters \\(\\theta\\) for our data. In other words, what we would actually like to compute is \\[Pr[\\theta|D].\\] This would allow us to select optimal parameter estimates and, importantly, to evaluate how good they are compared to other parameters. Getting from \\(Pr[D| \\theta]\\) to \\(Pr[\\theta|D]\\) can be solved in different ways, which has given rise to two major philosofical branches of statistics: Bayesian statistics and Frequentist statistics 4.1 Bayesian approach Lecture notes Bayes’ theorem (Thomas Bayes, 1702-1761) provides a way to obtain the requested \\(P[\\theta|X,Y]\\) \\[Pr[\\theta|D] = \\frac{Pr[D| \\theta]Pr[\\theta]}{Pr[D]}\\] Posterior probability \\(Pr[\\theta|D],\\) the probability, computed posterior to analysis, of the parameters \\(\\theta\\) conditioned on the observed data, i.e, our requested probability. An important characteristic of Bayesian statistics is that the focus is not on point estimates, but on the posterior probability distribution over the parameter space of \\(\\theta\\), which provides a measure of uncertainty (probabilities) in comparison to other values. Prior probability of \\(\\theta\\) \\(Pr[\\theta]\\) is the prior probability of \\(\\theta\\) and should according to Bayesian statistics reflect what we know (or believe to know) about how close \\(\\theta\\) is to the true parameters. We can use information from previous studies or we can assign a uninformative prior, e.g., \\(Pr[\\theta]\\) follows a uniform distribution for all \\(\\theta\\) in the interval \\([a,b]\\). It can be shown that the effect of the prior on the posterior probsbiity is largest when the observed data is small. With larger sample sizes, the posterior probability will eventually just depend on \\(Pr[D|\\theta]\\). Marginal Probability of \\(D\\) \\(Pr[D]=\\int_{\\theta}Pr[D| \\theta]Pr[\\theta]\\) is the probability of \\(D\\) regardless of \\(\\theta\\). This can often be difficult difficult to calculate and, for this reason, Bayesian models are often designed so that this can be calculated analystically or some approximation approach, such as Markov chain Monte Carlo (MCMC) is used. Extra reading Probabilistic algebra A conditional probability \\(Pr[A|B]\\) is the probability that \\(A\\) happens if we know that \\(B\\) has happened. To obtain the probability that both \\(A\\) and \\(B\\) happens we need to first take the probability that \\(B\\) happens and then multiply it with the conditional probability that \\(A\\) hapens given \\(B\\), i.e.,: \\[Pr[A,B] = Pr[A|B] Pr[B].\\] From this follows the reverse operation \\[\\frac{Pr[A,B]}{Pr[B]} = Pr{A|B}\\] Notice that this also works if we have more than one condition: \\(Pr[A|B,C] * Pr[B] = Pr[A,B|C].\\) What happens in Bayes rule is that we first, in the numerator, perform \\(Pr[B|A]*Pr[A] = Pr[A,B]\\) and then divide this with the denominator \\(\\frac{Pr[A,B]}{Pr[B]} = Pr[A|B]\\). 4.2 Likelihood – The frequentist approach Lecture notes Likelihood (Introduced by Fisher, 1925, formalized by Edwards, 1972) builds on the intuition that if \\(\\theta\\) is close to the ‘truth’, then \\(Pr[Y| X, \\theta]\\) will be higher than for wrong \\(\\theta\\). We should therefore select the \\(\\theta\\) that maximizes \\(Pr[Y| X, \\theta]\\); this is called maximum likelihood estimation (MLE) of \\(\\theta\\). Since statistical model contain an element of randomness, the reasoning above might not always be correct for any single obeservation. However, if we sum over a large number of observations it will be true on average. Hence the need for datasets that are large enough. To formalize this intuition, Edwards (1972) defined the likelihood of model parameters being true given observed data as \\[L[\\theta|D] \\propto Pr[D| \\theta]\\] Extra Reading Notice that this notation is not uncommonly mixed up, so you might also see the notation \\(L[Y|X,\\theta]\\) for the likelihood. Similarly \\(\\propto Pr[Y|X, \\theta]\\) is often referred to as the likelihood function. The proportionality (indicated by ‘\\(\\propto\\)’) means there are some unknown constant factor, \\(k\\), such that \\(L[\\theta|Y,X] = k Pr[Y|X, \\theta]\\). However, the factor \\(k\\) is assumed to be constant over \\(\\theta\\)s and over models. Using a Bayesian perspective, we can see that the proportionality constant \\(k = \\frac{Pr[\\theta]}{Pr[D]}\\), and that Likelihood would correspond to assuming a uniform prior over all possible values of \\(\\theta\\). In practice, the proportionality is ignored and we set \\[L[\\theta|Y,X] = Pr[Y|X, \\theta]\\] Extra Reading When the likelihood of two \\(\\theta\\)s (or models) are compared this is almost always done as a likelihood ratio, \\[\\frac{L[\\theta_1|Y,X]}{L[\\theta_0|Y,X]} = \\frac{k Pr[Y|X, \\theta_1]}{ k Pr[Y|X, \\theta_0]} =\\frac{Pr[Y|X, \\theta_1]}{ Pr[Y|X, \\theta_0]}\\] which means that the factor \\(k\\) disappears. Hence the factor \\(k\\) is always ignored. Likelihood ratios is the basis of most model comparison statistics, e.g., the Wald test, the Score test, regularization… In maximum likelihood estimation of some parameters \\(\\theta\\), one simply selects the estimates \\(\\widehat\\theta\\) that gives the highest likelihood, \\(max_{\\theta}L[\\theta|X,Y] = L[\\widehat\\theta|X,Y]\\). In many applications of likelihood and maximum likelihood, it is practical to instead use the logarithm of the likelihood, the logLikelihood, \\(\\log L[\\theta_1|Y,X]\\). Extra Reading As mentioned above, the logarithm of the likelihood, the logLikelihood, \\(\\log L[\\theta_1|Y,X]\\), or sometimes the negative logLikelihood, \\(-\\log L[\\theta_1|Y,X]\\), is often used. Notice, that The \\(\\theta\\) estimates that maximizes \\(\\log L[\\theta|Y,X]\\) also maximizes \\(L[\\theta|Y,X]\\) The \\(\\theta\\) estimates that minimizes \\(-\\log L[\\theta|Y,X]\\) maximizes \\(L[\\theta|Y,X]\\) A likelihood ratio corresponds to a logLikelihood difference, \\[\\log\\left(\\frac{L[\\theta_1|Y,X]}{L[\\theta_0|Y,X]}\\right) = \\frac{\\log L[\\theta_1|Y,X]}{\\log L[\\theta_0|Y,X]} = \\log L[\\theta_1|Y,X] - \\log L[\\theta_0|Y,X]\\]. Likelihood and maximum likelihood estimation are central concepts in frequentist statistics. Many statistical tests and methods uses or is based on the concept of maximum likelihood. In general, full-on likelihood computation and maximum likelihood estimation is relatively slow, so alternative and faster methods has been developed. One example is the use ordinary least squares OLS for linear models; it can be shown that the likelihood can be expressed as a function of the residual sum of squares (RSS) and that maximum likelihood estimates of \\(\\beta\\) is exactly the same as those of the OLS (which minimizes RSS. NB! This is a special case for linear models and are not generally true for other models. For example, logistic regression is typically fitted using maximizing the likelihood Extra Reading Linear models is a special case with some nice properties when it comes to likelihood. Consider a simple linear regression model, \\[ y = \\beta x + \\epsilon, \\] where the residuals \\(\\epsilon\\sim N(0,\\sigma^2)\\). It turns out that the likelihood estimates of both \\(\\beta\\) and \\(\\sigma^2\\) are functions of the RSS of the residuals, so that the likelihood can be approximated by \\[ \\log L[\\beta, \\sigma^2|Y,X] \\approx -\\frac{N}{2} \\log RSS\\] The likelihood for given \\(\\beta\\) and \\(\\sigma^2\\), given observed data \\(Y\\) and \\(X\\) is given by \\[ L[\\beta, \\sigma^2|Y,X] = \\prod_i pdf_{Normal}(y_i, \\mu=\\beta x_i, \\sigma^2=\\sigma^2) = \\prod_i \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(y_i-\\beta x_i)^2}{2\\sigma^&quot;}} \\] where \\(pdf_{Normal}\\) denotes the probability distribution function for the Normal distribution. If we work with the logLIkelihood instead, we get \\[\\begin{eqnarray*} \\log L[\\beta, \\sigma^2|Y,X] &amp;=&amp; \\sum_{i=1}^N \\log\\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(y_i-\\beta x_i)^2}{2\\sigma^2}}\\right)\\\\ &amp;=&amp; \\sum_{i=1}^N \\log \\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\right) -\\frac{(y_i-\\beta x_i)^2}{2\\sigma^2} \\\\ &amp;=&amp; N\\log \\left(2\\pi \\sigma^2\\right)^{-1/2} -\\frac{\\sum_{i=1}^N (y_i-\\beta x_i)^2}{2\\sigma^2} \\\\ &amp;=&amp; -\\frac{N}{2}\\log \\left(2\\pi \\sigma^2\\right) -\\frac{RSS}{2\\sigma^2} \\end{eqnarray*}\\] We see here that minimizing \\(RSS\\) (as in OLS) will maximize the logLikelihood, regardless of the value of \\(\\sigma^2\\). Moreover, it turns out that also \\(\\sigma^2\\) can be estimated fairly well by \\(RSS/N\\). Hence, we get \\[\\begin{eqnarray*} \\log L[\\beta, \\sigma^2|Y,X] &amp;=&amp; -\\frac{N}{2}\\log \\left(\\frac{2\\pi RSS}{N}\\right) -\\frac{N}{2}\\frac{RSS}{RSS}\\\\ &amp;=&amp; -\\frac{N}{2}\\log RSS + \\frac{N}{2}\\log \\frac{2\\pi}{N} -\\frac{N}{2}\\\\ &amp;=&amp; -\\frac{N}{2}\\log RSS + C \\end{eqnarray*}\\] where \\(C=\\frac{N}{2}\\left(\\log \\frac{2\\pi}{N} -1\\right)\\) is a constant that is usually ignored (in likelihood ratios, which is equivalent to log likelihoods differences, it will disappear). 4.3 Bayesians vs frequentists Lecture notes There is often described a severe controversy between Bayesians and frequentists. However, this controversy represents the extreme hardcore Bayesians and frequentists. In reality, there is a large gray-zone where frequentists and Bayesians meet and socialize: Bayesian models can be viewed as a type of the hierarchical models often used by frequentists Frequentist bootstrap analysis is often used to estimate uncertainty of point estimates in relation to alternatives, as is done in Bayesian statistics The Bayes factor is a Bayesian version of the likelihood ratio Bayesian posterior intervals corresponds to frequentist confidence intervals (Note however, that there are no Bayesian significance test) etc. Most practical statisticians use the tool that is adequate for the problem at hand, whether it is Bayesian or frequentist. "],
["overfitting.html", "5 Overfitting 5.1 Overfitting | Example data 5.2 Overfitting | Model comparison", " 5 Overfitting We will now look at a general problem in statistical modeling that can be visualized quite well with Likelihoods. We will later look at some solutions to this problem. 5.1 Overfitting | Example data First, you need some test data to play around with. For simplicity and convenience, you will simulate a toy data from a linear model and use this in the exercises. The advantage for us using simulated data is that we know the ‘truth’, i.e., how the data was simulated and we therefore have oracle knowledge about the true parameter values, e.g., for \\(\\beta\\). 5.1.1 Task | simulation of example data The data should comprise 100 samples. First generate 10 variables \\((x_1,x_2,\\ldots, x_{0})\\) from a uniform distribution (use the function runif) and store them in a Matrix \\(X\\). Use an intercept \\(\\beta_0=3\\) Generate effect sizes \\(\\beta_1, \\beta_2, \\beta_3\\) from a Uniform distribution in the interval \\((0.5, 1.0)\\) for the 3 first \\(X\\) variable (use the function runif); record the ‘true’ effect sizes for reference. Finally generate outcome variable \\(Y\\) using a linear model \\(Y = \\beta_0 + \\beta_1 x_i + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\), with \\(\\epsilon\\sim N(0,\\sigma^2=1)\\) (i.e., the residuals are drawn from a Normal distribution with mean=0 and standard deviation=1, Tip: use the R function rnorm). # To obtain exactly the same result as in the demo, set seed to 85 set.seed(85) N=100 # number of samples P=10 # number of variables # Draw variables, x_{i,1},...,x_{i,P} for all N individuals, from a uniform distribution in interval (0,1) (this is the default interval for runif) X=matrix(round(runif(N*(P+1),min=0, max=2)), nrow=N, ncol=P) # generate a y variable from a multivarite lm of 3 first X variables only # intercept b0=3 # effect sizes for first three variables b=c(runif(3, min=0.5, max=1.0)) # generate y Y = b0 + X[,1] * b[1] + X[,2] * b[2] + X[,3] * b[3] + rnorm(N) 5.2 Overfitting | Model comparison Now consider the following two linear models for our data \\[\\begin{eqnarray} y &amp; \\sim &amp; \\beta_0 + \\beta_1 x_1 &amp; (1) \\\\ y &amp; \\sim &amp; \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 &amp; (2) \\end{eqnarray}\\] What are the max Likelihood estimates of the two models? (we can use the R function logLik in the stats package) 5.2.1 Task | plot two likelihoods Create linear models (use lm) for the two models, and store the likelihood (use logLik) in a vector plot the likelihoods require(stats) ll= vector() for(i in seq(1,2)){ Xi=X[,seq(1,i)] ll[i] &lt;- logLik(lm(Y~Xi)) } # plot likelihoods for models with 1 and 2 vaiables plot(ll, ylab=&quot;log L&quot;, xlab=&quot;model #&quot;, type = &quot;b&quot;, xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) # xlim and ylim not really necessary here, but I can reuse the plot statement below, so the plots look similar Show result … 2 variables are clearly better than 1 variable – What if we add more variables? 5.2.2 Task | plot all likelihoods Now repeat this for the sequence of models obtained by creating the next model by simply adding the next \\(X\\) variable in order. # compute loglikelihood (ll) for all models including variables # 1-i, for i &lt;= P; store results in vector ll ll= vector() for(i in seq(1,P)){ Xi=X[,seq(1,i)] ll[i] &lt;- logLik(lm(Y~Xi)) } # plot ll for all models plot(ll, ylab=&quot;log L&quot;, xlab=&quot;model #&quot;, type = &quot;b&quot;, xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) Show result 5.2.2.1 Think about: How does the Likelihood behave as more variables are added? Which is the maximum likelihood model? Is this correct given our oracle knowledge? What could be the problem with this behaviour? How would we like it to behave? How can we obtain the desired behaviour? Some possible answers Some possible answers Nested models Model (1) can be described as a special case of Model (2) with the constraints on \\(\\beta_2=0\\) Therefore Model (2) will always have equal or better ML than Model (1) We say that model (1) is nested in Model (2) (which is nested in Model (3) etc.) Overfitting Using our oracle knowledge, we know that the simulated data was generated from the 3 first variables thus, the subsequent variables increase ML by modeling noise in data This is difficult to detect by just looking at the likelihoods Solutions Seek the simplest model that is “good enough” -&gt; Regularization/Bayesian Extra Reading Model comparison | Likelihood ratio test (Read Extra reading about likelihood ratios under 4.2 first) For nested models \\(-2 \\max LRT\\) is \\(\\chi^2(d)\\)-distributed, with \\(d=\\) the difference in free params in the two models. Compared models logL 1st model logL 2nd model logLR P-value Sign at 0.05 1 vs 2 variables -155.80 -146.90 -8.9 2.45e-05 yes 2 vs 3 variables -146.90 -136.73 -10.17 6.48e-06 yes 3 vs 4 variables -136.73 -136.69 -0.04215 0.772 no 4 vs 5 variables -136.69 -136.23 -0.4601 0.337 no 5 vs 6 variables -136.23 -135.83 -0.4016 0.37 no 6 vs 7 variables -135.83 -135.35 -0.4803 0.327 no 7 vs 8 variables -135.35 -135.31 -0.04266 0.77 no 8 vs 9 variables -135.31 -135.30 -0.0002981 0.981 no 9 vs 10 variables -135.30 -134.55 -0.7536 0.22 no In our simple test case, the LRT also succeed in picking the correct model. It should be noted that certain issues, such as lnkage disequilibriium, may cause problems for LRT (the example is not optimized to show this). "],
["regularization.html", "6 Regularization 6.1 Regularization | Bayesian interpretation 6.2 Regularization | AIC and model testing 6.3 Regularization | LASSO and Feature selection 6.4 Cross-validation", " 6 Regularization Lecture notes Regularization is a concept that adds auxiliary criteria, so-called regularization terms, to probabilistic models. This is called regularized likelihood models or penalized likelihood models. Typically, the regularization term is a function of parameters \\(\\beta\\): \\[\\log rL[\\beta | X, Y] = \\log Pr[Y | X, \\beta] - f(\\beta),\\] A very simple regularized likelihood model uses \\(f(\\beta) = \\#\\beta = \\#X\\), that is the number of \\(X\\) variables. \\[\\log rL[{\\beta} | X, Y] = \\log Pr[Y | X, {\\beta}] - \\#X, \\] Applying this rL to our example data, solves the overfitting problem. # compute loglikelihood (ll) for all models including 1-P variables pl= vector() for(i in seq(1,P)){ xi=X[,seq(1,i)] xi=cbind(rep(1,N), xi) fit = lm(Y~xi) # To make the code simple, we forestall next step and use the AIC function here # AIC= -2(pl) so convert back pl[i] = -AIC(fit)/2 } # plot ll of all models plot(pl, xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab=&quot;log pL&quot;, xlab=&quot;model #&quot;, type = &quot;b&quot;) 6.1 Regularization | Bayesian interpretation Lecture notes Regularization is a canonical example where Bayesian and frequentist statistics meet. The standard way of writing a regularized likelihood is using the logLikelihood, but what if ‘de-log’ it: \\[\\begin{eqnarray*} \\log rL[\\beta | X, Y] &amp;=&amp; \\log Pr[Y | X, \\beta] - f(\\beta) \\\\ \\Downarrow\\\\ rL[\\beta | X, Y] &amp;=&amp; Pr[Y | X, \\beta] * e^{- f(\\beta)} \\end{eqnarray*}\\] This looks suspiciously like an un-normalized posterior probability (i.e., lacking the denominator), with an exponential prior \\(Pr[\\beta]=e^{-f(\\beta)}.\\) As we will see examples of, most regularization techniques have a Bayesian interpretation. In fact, a standard solution overfitting and, more generally, over-parameterization, i.e., problems where the likelihood function may not have a unique maximmum, is to include prior information, either as Bayesian priors or regularization terms to limit the parameter space. This is an area where Bayesian and frequentist socialize and get on well. 6.2 Regularization | AIC and model testing Lecture notes Coming from a information theory base, Hirotugu Akaike (1974) came up with a very similar approach for solving the overfitting problem. The Akaike information criterion (AIC), for a model \\(m\\) with variables \\(X\\), is defined as \\[AIC_m = 2\\# X - 2\\log \\max L[{\\beta}|X,Y]\\] We see that \\(AIC_m = -2 \\left(\\log \\max L[{\\beta}|X,Y] - \\#X\\right)\\), i.e., \\(-2\\) times the the simple \\(\\log rL\\), we just looked at in our first regularization example. In the information theory context, the difference in \\(AIC\\) between two models is claimed to estimate the information lost by selecting the worse model. Sometimes, the relative likelihood for model \\(m\\) is used, which is \\[relL = e^\\frac{ AIC_{min} - AIC_{m} }{2}\\] where \\(AIC_{min}\\) is the minimum AIC among a set of compared models Extra Reading \\(relL\\) can be interpreted as proportional to the probability that the model \\(m\\) minimizes the information loss. Notice that\\[ relL = \\frac{e^{\\#X_m} }{e^{\\#X_{min}}}\\frac{\\max L[{\\beta}_{m}|X_m,Y]}{\\max L[{\\beta}_{min}|X_{min},Y]}\\] we see that \\(rL\\) can be viewed as a weighted likelihood ratio or maybe more naturally as a Bayes factor 6.2.1 Task | AIC analysis A typical AIC analyss strategy is to select the model, \\(m\\) with \\(AIC_m=AIC_{min}\\) and then evaluate how much better it is than the other candidate models, e.g., using the \\(relL\\). Apply this AIC strategy applied to our example data using the R funcion AIC create a table with the AIC and the \\(relL\\) for the set of models comprising \\(\\{X_1, .\\ldots, X_i\\} \\textrm{ for } i \\in [1, \\ldots, 10]\\); indicate also if a model is the minimum AIC model. require(stats) require(dplyr) # used for nice table formatting require(kableExtra) # used for nice table formatting mprev &lt;- lm(Y ~ X[,1]) # current miminimum AIC model # dummyentry to be replaced aic=data.frame(models=0, aic=0, isAICmin=&quot;-&quot;) for(i in seq(2,P)){ m &lt;- lm(Y ~ X[,seq(1,i)]) fit = AIC(mprev,m) # compare with current the minimum model mprev = m if(i==2){ #include also the first model aic[i-1,] = list(paste0(i-1,&quot; variable&quot;), signif(fit$AIC[1],5), &quot;-&quot;) } aic[i,] = list(paste0(i,&quot; variables&quot;), signif(fit$AIC[2],5), &quot;-&quot;) } minaic=min(aic$aic) aic$rl=format(exp((minaic-aic$aic)/2), digits=4) aic$isAICmin = ifelse(aic$aic==minaic,&quot;Yes&quot;,&quot;-&quot;) kable(aic, format=&#39;html&#39;, row.names=F, col.names=c(&quot;Compared models&quot;,&quot;AIC&quot;,&quot;Minimum AIC&quot;,&quot;rL&quot;),digits=30,format.args=list(snsmall=0)) %&gt;% kable_styling( font_size = 14) Show result Compared models AIC Minimum AIC rL 1 variable 317.61 3.841e-08 2 variables 301.80 1.041e-04 3 variables 283.46 Yes 1.000e+00 4 variables 285.38 3.829e-01 5 variables 286.46 2.231e-01 6 variables 287.66 1.225e-01 7 variables 288.70 7.280e-02 8 variables 290.61 2.802e-02 9 variables 292.61 1.031e-02 10 variables 293.10 8.067e-03 Try to plot the \\(AIC\\) and the \\(reL\\) with the different models on the \\(X\\)-axis require(stats) # plot AIC of all models plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(aic$aic)),ceiling(max(aic$aic))),ylab=&quot;AIC&quot;, xlab=&quot;model #&quot;, type = &quot;b&quot;) # plot relL of all models plot(aic$rl, xlim=c(1,P), ylab=&quot;relL&quot;, xlab=&quot;model #&quot;, type = &quot;b&quot;) Show result 6.2.1.1 Think about: Which is the best model? Is this correct compared to our oracle knowledge? How good is it compared to the others? Can you see a drawback in our model testing approach above? If so, how can we solve that? Some possible answers Some possible answers We see that the best model is the one with the 3 first X-variables (in line with our oracle knowledge) and that the second best model (with the first 4 X-variabels) is \\(\\approx 60\\%\\) as good. Extra Reading Sometimes it is desirable to compute a significance for rejecting a model in favour of another model. A NULL distribution for the \\(relL\\) statistic is usally obtained through simulation, e.g., using parameteric bootstrapping. Now, I this case we happened to know that the first 3 variables was the right one, so the order we choose to include them was correct. However, in the general case, we do not know this. How solve this? Best subset method; involves testing all possible subsets, which is computationally time-consuming and sometimes unfeasible Lasso 6.3 Regularization | LASSO and Feature selection Lecture notes LASSO stands for Least absolute shrinkage and selection operator (“shrinkage” is another common term for regularization) and is a method for selecting variables to include in a multivariate model. Classical LASSO builds on RSS of a linear regression model \\(Y \\sim X{\\beta}\\) with regularization Extra Reading Extensions to glms exists, but then using a regularized likelihood expression The regularization term \\(f(\\beta) = \\lambda\\sum_{\\beta_i\\in\\beta} |\\beta_i-0|= \\lambda\\sum_{\\beta_i\\in\\beta} |\\beta_i|\\) The \\(\\lambda\\) parameter sets a limit on the estimation of \\(\\beta\\). Lasso is traditionally described as RSS with an auxiliary criterion/constraint: \\[min_{{\\beta}}\\left\\{RSS\\right\\} + \\lambda\\sum_{\\beta_i\\in\\beta} |\\beta_i|.\\] Lasso can also be viewed as a un-normalized Bayesian posterior probability, with a LaPlacean prior on \\(\\beta\\): \\(\\beta_j ∼ LaPlace(0, 1/\\lambda)\\) Extra Reading Often the regularization term is expressed in terms of the \\(\\ell_1-norm\\), which can be viewed simply a short-hand notation, e.g., the \\(\\ell_1-norm\\) of \\(\\beta\\) is \\[ ||\\beta||_1 = \\sum_{\\beta_i\\in\\beta} |\\beta_i|\\] There is also a \\(\\ell_2-norm\\): \\[ ||\\beta||_2 = \\sqrt{\\sum_{\\beta_i\\in{\\beta}} \\beta_i^2}\\] which is used, e.g., in ridge regression. This correspond to a Normal prior for Baysians. We note, BTW, that you might already have encountered an \\(\\ell_2-norm\\): since \\(RSS = ||Y-X\\beta||_2^2\\) is simply the square of the \\(\\ell_2\\) norm of the residuals. You might also see the notation \\[min_{{\\beta}}\\left\\{RSS\\right\\} \\textrm{ subject to } ||{\\beta}||_1 &lt;= t\\] where \\(t\\) is related to \\(\\lambda\\). The optimal values of \\(\\beta\\) for different values of \\(\\lambda\\) are then estimated, using some algorithm (lars or coordinate descent). A convenient way to think about this is that at very high \\(\\lambda\\) values, all \\(\\beta_s\\) are 0; by sequentially lowering \\(\\lambda\\) more and more \\(\\beta_i\\) become non-zero, the most important variables \\(i\\) are included first. Extra Reading The Coordinate descent algorithm is used in the R package glmnet: Over a grid of \\(\\lambda\\in [0, \\infty]\\), do Start with all \\(\\beta=0\\) until convergence repeat for each \\(\\beta_i\\) while keeping all other \\(\\beta\\) fixed and \\(\\beta_i=0\\), compute partial residuals estimate \\(\\beta_i\\) by RSS on the partial residuals update $_i using the RSS estimate and \\(\\lambda\\). Extra Reading Alternatives to LASSO, differing mainly in the auxiliary criterion Ridge regression which uses a \\(\\ell_2\\) norm Elastic-net, which uses a mixed model combination of the \\(\\ell_1\\) norm and the \\(\\ell_2\\) norm. 6.3.1 Task | Lasso using the glmnet R-package Use function glmnet to perform LASSO analysis on our example data; relevant arguments of the function include: linear regression (family='gaussian' = default) LASSO (alpha=1 = default) standardization The variables Y and X must be centered and standardized to ensure that all variables are given equal weight in the model selection. standardization of \\(X\\) to unit variance in glmnet is obtained by setting the argument standardize=TRUE which is the default the values of \\(Y\\) is always standardized (?) for family=gaussian (LASSO) and the coefficients are back-standardized before reported Extra Reading Standardization in glmnet: \\(x&#39; = \\frac{x-\\bar{x}}{s/\\sqrt{N}}\\) where \\(\\frac{s}{\\sqrt{N}}\\) is the estimate of the standard deviation of \\(x\\) (and, incidently, can be written using the \\(\\ell_2-\\)norm: \\(\\frac{s}{\\sqrt{N}} = \\frac{\\sum_{x_i \\in x} (x-\\bar{x})^2}{\\sqrt{N}} = \\frac{||X-\\bar{x}||_2}{\\sqrt{N}}\\)). require(glmnet) # run lasso (alpha=1) for linear model (family=gaussian) fit = glmnet(X,Y, family=&quot;gaussian&quot;, alpha=1, standardize=T) A graphical way to view the result is to plot the paths of \\(\\beta\\) for increasing vaules of \\(\\lambda\\). plot(fit, xvar=&quot;lambda&quot;,label=T) Show result 6.3.1.1 Think about In which order are variables included (i.e., their \\(\\beta\\) becomes non-zero)? In which direction is the effect Which lambda should we select? Given our oracle knowledge, where would an appropriate \\(\\lambda\\) be? Can we use that? Some possible answers Some possible answers The order in the above plot appears to be \\((1,2,3,7,6,5,10,9,4,8)\\) (may vary depending on the simulation) \\(\\beta_i &gt; 0, i\\in \\{1,2,3,4,7,9\\}\\), while \\(\\beta_i&lt;0, i\\in \\{5,6,8,10\\}\\) Given oracle knowledge, the correct \\(\\lambda\\) appears lie somewhere in the interval \\([\\approx \\exp(-0.9), \\approx\\exp(-2.25)]\\) In the typical analysis case, we never have oracle knowledge. 6.4 Cross-validation Lecture notes The LASSO model will be different depending on how we set \\(\\lambda\\). A problem is to decide the optimal \\(\\lambda\\) to use. \\(\\lambda\\) too high: risk of missing relevant variables \\(\\lambda\\) too low: risk of overfitting glmnet addresses this using \\(k\\)-fold cross-validation – what is that? 6.4.1 Cross-validation | How to test for overfitting Lecture notes The ultimate way of testing an estimated model (with parameters) is to apply it to new data and evaluate how well it performs, e.g., by measuring the mean squared error, \\(MSE\\) (\\(=RSS/N\\)). Naturally, we want to minimize \\(MSE\\), i.e., the error of the model. In our LASSO application, this means that we want to select the \\(\\lambda\\) that minimizes the \\(MSE\\) In cross validation, this approach is emulated by partioning the data at hand into a training and a validation data set. The model parameters are estimated (‘trained’) on the the training data and the validated on the test data. By chance, this may fail if the partitioning is ‘non-representative’. A solution is to repeat the cross-validation procedure with another partioning. In \\(k\\)-fold cross validation, the original data is split into \\(k\\) sub-datasets \\(\\{D_1,D_2,\\ldots, D_k\\}\\). For \\(i \\in \\{1,2,\\ldots, k\\}\\), set \\(D_i\\) as the test data set and the union of the other datasets be the training data. Perform cross validation as above. This gives a distribution of \\(MSE\\) from which we can estimate, e.g., mean and standard deviation. Additional reading This distribution allows us to use more elaborate means to select \\(\\lambda\\). One common suggestion is to use the largest \\(\\lambda\\) whose \\(MSE\\) is within 1 standard error from the minimum value (called lambda.1se in glmnet). The motivation argued for this choice is parsimony, in the sense that larger \\(\\lambda\\) will include fewer variables (hence it is parsimonious in terms of number of included variables). Here we will limit ourselves to finding the minimum \\(\\lambda\\), called lambda.min in glmnet, but anyone is free to test if lambda.1se gives a different result. 6.4.2 Task | Determine optimal LASSO\\(\\lambda\\)using cross-validation Use the function cv.glmnet to perform cross validation (same options as for glmnet) plot the cross-validation results Compare with the plot of estimated \\(\\beta_i\\) under different \\(\\lambda\\). Determine the optimal \\(\\lambda\\) (the one with minimal error) require(glmnet) par(mfrow=c(1,1)) # run lasso (alpha=1) for linear model (family=gaussian) cvglm=cv.glmnet(X,Y, family=&quot;gaussian&quot;, alpha=1, standardize=T, nfolds=100) plot(cvglm) plot(cvglm$glmnet.fit, xvar=&quot;lambda&quot;,label=T) minlambda=cvglm$lambda.min Show result 6.4.2.1 Think about Which is the \\(\\lambda\\) selected by cv.glmnet? Does this make sense given our oracle knowledge? Some possible answers Some possible answers Cross-validation-selected optimal \\(\\lambda\\) is 0.1064891 (\\(log \\lambda=\\)-2.2397124) Yes, this includes only the oracle knowledge correct variables \\(X_1, X_2, X_3\\) 6.4.3 Task| Final LASSO effect sizes Finally print a table with the \\(\\beta\\) coefficients (including the intercept, \\(\\beta_0\\)) for the optimal model (i.e., at minimum \\(\\lambda\\)). (Use functioncoef). # Actually the following suffice for output on console #coef(cvglm, s=&quot;lambda.min&quot;) # But to get a nice table: require(dplyr) # for nice table require(kableExtra) #for nice table coefglm=as.data.frame(as.matrix(coef(cvglm, s=&quot;lambda.min&quot;))) coefglm=cbind(seq(0,10),c(b0, b, rep(0, 7)),coefglm) names(coefglm)=c(&quot;beta&quot;,&quot;value (oracle)&quot;, paste0(&quot;estimate(lambda=&quot;,signif(minlambda,3),&quot;)&quot;)) kable(coefglm, row.names=F) %&gt;% kable_styling( font_size = 14) Show result beta value (oracle) estimate(lambda=0.106) 0 3.0000000 3.6046135 1 0.8125118 0.5905522 2 0.6009469 0.4631531 3 0.7232911 0.5204561 4 0.0000000 0.0000000 5 0.0000000 0.0000000 6 0.0000000 0.0000000 7 0.0000000 0.0000000 8 0.0000000 0.0000000 9 0.0000000 0.0000000 10 0.0000000 0.0000000 6.4.3.1 Think about Does the effect sizes make sense – if not can you think of why? Some possible answers Some possible answers Well…yes… sort of! \\(\\beta_i\\) is non-zero only for oracle-known variables \\(X_1, X_2, X_3\\) however,they don’t exactly equate our oracle knowledge parameter values – they appear to be scaled. but their relative order of amplitude is right. Perhaps the normalization affected scaling. "],
["session-info-1.html", "7 Session info", " 7 Session info ## R version 3.5.3 (2019-03-11) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] glmnet_2.0-18 foreach_1.4.4 Matrix_1.2-17 dplyr_0.8.1 ## [5] kableExtra_1.1.0 lmtest_0.9-37 zoo_1.8-6 captioner_2.2.3 ## [9] igraph_1.2.4.1 bookdown_0.11 knitr_1.23 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_0.2.5 xfun_0.7 purrr_0.3.2 ## [4] lattice_0.20-38 colorspace_1.4-1 htmltools_0.3.6 ## [7] viridisLite_0.3.0 yaml_2.2.0 rlang_0.3.4 ## [10] later_0.8.0 pillar_1.4.1 glue_1.3.1 ## [13] stringr_1.4.0 munsell_0.5.0 rvest_0.3.4 ## [16] codetools_0.2-16 evaluate_0.14 httpuv_1.5.1 ## [19] highr_0.8 Rcpp_1.0.1 readr_1.3.1 ## [22] promises_1.0.1 scales_1.0.0 webshot_0.5.1 ## [25] jsonlite_1.6 mime_0.6 servr_0.13 ## [28] hms_0.4.2 packrat_0.5.0 digest_0.6.19 ## [31] stringi_1.4.3 xaringan_0.10 grid_3.5.3 ## [34] tools_3.5.3 magrittr_1.5 tibble_2.1.3 ## [37] crayon_1.3.4 pkgconfig_2.0.2 rsconnect_0.8.13 ## [40] xml2_1.2.0 assertthat_0.2.1 rmarkdown_1.13 ## [43] httr_1.4.0 rstudioapi_0.10 iterators_1.0.10 ## [46] R6_2.4.0 compiler_3.5.3 Built on: 14-Jun-2019 at 11:31:05. 2019 • SciLifeLab • NBIS • RaukR "]
]
