<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Regularization | Lab: Basic Statistics | Models</title>
  <meta name="description" content="6 Regularization | Lab: Basic Statistics | Models" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Regularization | Lab: Basic Statistics | Models" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Regularization | Lab: Basic Statistics | Models" />
  
  
  

<meta name="author" content="Bengt Sennblad" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="overfitting.html">
<link rel="next" href="session-info-1.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<link id="font-awesome-1-attachment" rel="attachment" href="libs/font-awesome-5.1.0/fonts/fontawesome-webfont.ttf"/>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="tasks.html"><a href="tasks.html"><i class="fa fa-check"></i><b>1</b> Tasks</a><ul>
<li class="chapter" data-level="1.1" data-path="tasks.html"><a href="tasks.html#task-simulation"><i class="fa fa-check"></i><b>1.1</b> Task | <code>Simulation</code></a><ul>
<li class="chapter" data-level="1.1.1" data-path="tasks.html"><a href="tasks.html#deterministic-model"><i class="fa fa-check"></i><b>1.1.1</b> Deterministic model</a></li>
<li class="chapter" data-level="1.1.2" data-path="tasks.html"><a href="tasks.html#statistical-model"><i class="fa fa-check"></i><b>1.1.2</b> Statistical model</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="tasks.html"><a href="tasks.html#task-probability-of-observed-data"><i class="fa fa-check"></i><b>1.2</b> Task | <code>Probability of observed data</code></a><ul>
<li class="chapter" data-level="1.2.1" data-path="tasks.html"><a href="tasks.html#simulation-solution"><i class="fa fa-check"></i><b>1.2.1</b> Simulation solution</a></li>
<li class="chapter" data-level="1.2.2" data-path="tasks.html"><a href="tasks.html#analytical-solution"><i class="fa fa-check"></i><b>1.2.2</b> Analytical solution</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="tasks.html"><a href="tasks.html#task-statistical-tests"><i class="fa fa-check"></i><b>1.3</b> Task | <code>Statistical tests</code></a><ul>
<li class="chapter" data-level="1.3.1" data-path="tasks.html"><a href="tasks.html#students-t-test"><i class="fa fa-check"></i><b>1.3.1</b> Student’s t-test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>2</b> Final words</a><ul>
<li class="chapter" data-level="2.1" data-path="final-words.html"><a href="final-words.html#types-of-models"><i class="fa fa-check"></i><b>2.1</b> Types of models</a></li>
<li class="chapter" data-level="2.2" data-path="final-words.html"><a href="final-words.html#types-of-statistical-tests"><i class="fa fa-check"></i><b>2.2</b> Types of statistical tests</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="session-info.html"><a href="session-info.html"><i class="fa fa-check"></i><b>3</b> Session info</a></li>
<li class="chapter" data-level="4" data-path="parameter-estimation-of-statistical-models.html"><a href="parameter-estimation-of-statistical-models.html"><i class="fa fa-check"></i><b>4</b> Parameter estimation of statistical models</a><ul>
<li class="chapter" data-level="4.1" data-path="parameter-estimation-of-statistical-models.html"><a href="parameter-estimation-of-statistical-models.html#bayesian-approach"><i class="fa fa-check"></i><b>4.1</b> Bayesian approach</a></li>
<li class="chapter" data-level="4.2" data-path="parameter-estimation-of-statistical-models.html"><a href="parameter-estimation-of-statistical-models.html#likelihood"><i class="fa fa-check"></i><b>4.2</b> Likelihood – The frequentist approach</a></li>
<li class="chapter" data-level="4.3" data-path="parameter-estimation-of-statistical-models.html"><a href="parameter-estimation-of-statistical-models.html#bayesians-vs-frequentists"><i class="fa fa-check"></i><b>4.3</b> Bayesians vs frequentists</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>5</b> Overfitting</a><ul>
<li class="chapter" data-level="5.1" data-path="overfitting.html"><a href="overfitting.html#overfitting-example-data"><i class="fa fa-check"></i><b>5.1</b> Overfitting | <code>Example data</code></a><ul>
<li class="chapter" data-level="5.1.1" data-path="overfitting.html"><a href="overfitting.html#task-simulation-of-example-data"><i class="fa fa-check"></i><b>5.1.1</b> Task | <code>simulation of example data</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="overfitting.html"><a href="overfitting.html#overfitting-model-comparison"><i class="fa fa-check"></i><b>5.2</b> Overfitting | <code>Model comparison</code></a><ul>
<li class="chapter" data-level="5.2.1" data-path="overfitting.html"><a href="overfitting.html#task-plot-two-likelihoods"><i class="fa fa-check"></i><b>5.2.1</b> Task | <code>plot two likelihoods</code></a></li>
<li class="chapter" data-level="5.2.2" data-path="overfitting.html"><a href="overfitting.html#task-plot-all-likelihoods"><i class="fa fa-check"></i><b>5.2.2</b> Task | <code>plot all likelihoods</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>6</b> Regularization</a><ul>
<li class="chapter" data-level="6.1" data-path="regularization.html"><a href="regularization.html#regularization-bayesian-interpretation"><i class="fa fa-check"></i><b>6.1</b> Regularization | <code>Bayesian interpretation</code></a></li>
<li class="chapter" data-level="6.2" data-path="regularization.html"><a href="regularization.html#regularization-aic-and-model-testing"><i class="fa fa-check"></i><b>6.2</b> Regularization | <code>AIC and model testing</code></a><ul>
<li class="chapter" data-level="6.2.1" data-path="regularization.html"><a href="regularization.html#task-aic-analysis"><i class="fa fa-check"></i><b>6.2.1</b> Task | <code>AIC analysis</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regularization.html"><a href="regularization.html#regularization-lasso-and-feature-selection"><i class="fa fa-check"></i><b>6.3</b> Regularization | <code>LASSO and Feature selection</code></a><ul>
<li class="chapter" data-level="6.3.1" data-path="regularization.html"><a href="regularization.html#task-lasso-using-the-glmnet-r-package"><i class="fa fa-check"></i><b>6.3.1</b> Task | <code>Lasso using the glmnet R-package</code></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="regularization.html"><a href="regularization.html#cross-validation"><i class="fa fa-check"></i><b>6.4</b> Cross-validation</a><ul>
<li class="chapter" data-level="6.4.1" data-path="regularization.html"><a href="regularization.html#cross-validation-how-to-test-for-overfitting"><i class="fa fa-check"></i><b>6.4.1</b> Cross-validation | <code>How to test for overfitting</code></a></li>
<li class="chapter" data-level="6.4.2" data-path="regularization.html"><a href="regularization.html#task-determine-optimal-lassolambdausing-cross-validation"><i class="fa fa-check"></i><b>6.4.2</b> Task | <code>Determine optimal LASSO</code><span class="math inline">\(\lambda\)</span><code>using cross-validation</code></a></li>
<li class="chapter" data-level="6.4.3" data-path="regularization.html"><a href="regularization.html#task-final-lasso-effect-sizes"><i class="fa fa-check"></i><b>6.4.3</b> Task| <code>Final LASSO effect sizes</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="session-info-1.html"><a href="session-info-1.html"><i class="fa fa-check"></i><b>7</b> Session info</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lab: Basic Statistics | <code>Models</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularization" class="section level1">
<h1><span class="header-section-number">6</span> Regularization</h1>
<details>
<p><summary> Lecture notes </summary></p>
<p>Regularization is a concept that adds auxiliary criteria, so-called <em>regularization terms</em>, to probabilistic models. This is called regularized likelihood models or penalized likelihood models. Typically, the regularization term is a function of parameters <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[\log rL[\beta | X, Y]  = \log Pr[Y | X, \beta] - f(\beta),\]</span></p>
<p>A very simple regularized likelihood model uses <span class="math inline">\(f(\beta) = \#\beta = \#X\)</span>, that is the number of <span class="math inline">\(X\)</span> variables.<br />
<span class="math display">\[\log rL[{\beta} | X, Y]  = \log Pr[Y | X, {\beta}] - \#X, \]</span></p>
<p>Applying this rL to our example data, solves the overfitting problem.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute loglikelihood (ll) for all models including 1-P variables</span>
pl=<span class="st"> </span><span class="kw">vector</span>() 
<span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq</span>(<span class="dv">1</span>,P)){
  xi=X[,<span class="kw">seq</span>(<span class="dv">1</span>,i)]
  xi=<span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,N), xi)
  fit =<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>xi)
  <span class="co"># To make the code simple, we forestall next step and use the AIC function here</span>
  <span class="co"># AIC= -2(pl) so convert back</span>
  pl[i] =<span class="st"> </span><span class="op">-</span><span class="kw">AIC</span>(fit)<span class="op">/</span><span class="dv">2</span>
}
<span class="co"># plot ll of all models</span>
<span class="kw">plot</span>(pl, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">1</span>,P), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="kw">floor</span>(<span class="kw">min</span>(pl)),<span class="kw">ceiling</span>(<span class="kw">max</span>(pl))),<span class="dt">ylab=</span><span class="st">&quot;log pL&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;model #&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-22-1.svg" width="672" style="display: block; margin: auto auto auto 0;" /></p>
</details>
<div id="regularization-bayesian-interpretation" class="section level2">
<h2><span class="header-section-number">6.1</span> Regularization | <code>Bayesian interpretation</code></h2>
<details>
<p><summary> Lecture notes </summary></p>
<p>Regularization is a canonical example where Bayesian and frequentist statistics meet.</p>
<p>The standard way of writing a regularized likelihood is using the logLikelihood, but what if ‘de-log’ it:</p>
<p><span class="math display">\[\begin{eqnarray*}
\log rL[\beta | X, Y]  &amp;=&amp; \log Pr[Y | X, \beta] - f(\beta) \\
\Downarrow\\
rL[\beta | X, Y]  &amp;=&amp; Pr[Y | X, \beta] * e^{- f(\beta)}
\end{eqnarray*}\]</span></p>
<p>This looks suspiciously like an un-normalized posterior probability (i.e., lacking the denominator), with an exponential prior <span class="math inline">\(Pr[\beta]=e^{-f(\beta)}.\)</span></p>
<p>As we will see examples of, most regularization techniques have a Bayesian interpretation.</p>
<p>In fact, a standard solution overfitting and, more generally, over-parameterization, i.e., problems where the likelihood function may not have a unique maximmum, is to include prior information, either as Bayesian priors or regularization terms to limit the parameter space. This is an area where Bayesian and frequentist socialize and get on well.</p>
</details>
</div>
<div id="regularization-aic-and-model-testing" class="section level2">
<h2><span class="header-section-number">6.2</span> Regularization | <code>AIC and model testing</code></h2>
<details>
<p><summary> Lecture notes </summary></p>
<p>Coming from a <em>information theory</em> base, Hirotugu Akaike (1974) came up with a very similar approach for solving the overfitting problem.</p>
<p>The Akaike information criterion (AIC), for a model <span class="math inline">\(m\)</span> with variables <span class="math inline">\(X\)</span>, is defined as</p>
<p><span class="math display">\[AIC_m = 2\# X - 2\log \max L[{\beta}|X,Y]\]</span></p>
<p>We see that <span class="math inline">\(AIC_m = -2 \left(\log \max L[{\beta}|X,Y] - \#X\right)\)</span>, i.e., <span class="math inline">\(-2\)</span> times the the simple <span class="math inline">\(\log rL\)</span>, we just looked at in our first regularization example.</p>
<p>In the information theory context, the difference in <span class="math inline">\(AIC\)</span> between two models is claimed to estimate the information lost by selecting the worse model.</p>
<p>Sometimes, the <em>relative likelihood</em> for model <span class="math inline">\(m\)</span> is used, which is
<span class="math display">\[relL = e^\frac{ AIC_{min} - AIC_{m} }{2}\]</span>
where <span class="math inline">\(AIC_{min}\)</span> is the minimum AIC among a set of compared models</p>
<details>
<p><summary> Extra Reading </summary></p>
<ul>
<li><p><span class="math inline">\(relL\)</span> can be interpreted as proportional to the probability that the model <span class="math inline">\(m\)</span> minimizes the information loss.
<!--       and can be interpreted as -->
<!-- $rL \propto Pr[m\textrm{ minimizes estimated information loss}]$. --></p>
<ul>
<li>Notice that<span class="math display">\[ relL = \frac{e^{\#X_m} }{e^{\#X_{min}}}\frac{\max L[{\beta}_{m}|X_m,Y]}{\max L[{\beta}_{min}|X_{min},Y]}\]</span>
we see that <span class="math inline">\(rL\)</span> can be viewed as a weighted likelihood ratio or maybe more naturally as a Bayes factor</li>
</ul></li>
</ul>
<hr />
</details>
</details>
<div id="task-aic-analysis" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Task | <code>AIC analysis</code></h3>
<ul>
<li><p>A typical AIC analyss strategy is to select the model, <span class="math inline">\(m\)</span> with <span class="math inline">\(AIC_m=AIC_{min}\)</span> and then evaluate how much better it is than the other candidate models, e.g., using the <span class="math inline">\(relL\)</span>.</p></li>
<li>Apply this AIC strategy applied to our example data using the R funcion <code>AIC</code></li>
<li><p>create a table with the AIC and the <span class="math inline">\(relL\)</span> for the set of models comprising <span class="math inline">\(\{X_1, .\ldots, X_i\} \textrm{ for } i \in [1, \ldots, 10]\)</span>; indicate also if a model is the minimum AIC model.</p></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(stats) 
<span class="kw">require</span>(dplyr)      <span class="co"># used for nice table formatting</span>
<span class="kw">require</span>(kableExtra) <span class="co"># used for nice table formatting</span>

mprev &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X[,<span class="dv">1</span>]) <span class="co"># current miminimum AIC model</span>
<span class="co"># dummyentry to be replaced</span>
aic=<span class="kw">data.frame</span>(<span class="dt">models=</span><span class="dv">0</span>, <span class="dt">aic=</span><span class="dv">0</span>, <span class="dt">isAICmin=</span><span class="st">&quot;-&quot;</span>) 

<span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq</span>(<span class="dv">2</span>,P)){
  m &lt;-<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X[,<span class="kw">seq</span>(<span class="dv">1</span>,i)])
  fit =<span class="st"> </span><span class="kw">AIC</span>(mprev,m) <span class="co"># compare with current the minimum model</span>
  mprev =<span class="st"> </span>m
  <span class="cf">if</span>(i<span class="op">==</span><span class="dv">2</span>){ <span class="co">#include also the first model</span>
    aic[i<span class="dv">-1</span>,] =<span class="st"> </span><span class="kw">list</span>(<span class="kw">paste0</span>(i<span class="dv">-1</span>,<span class="st">&quot; variable&quot;</span>), <span class="kw">signif</span>(fit<span class="op">$</span>AIC[<span class="dv">1</span>],<span class="dv">5</span>), <span class="st">&quot;-&quot;</span>) 
  }
  aic[i,] =<span class="st"> </span><span class="kw">list</span>(<span class="kw">paste0</span>(i,<span class="st">&quot; variables&quot;</span>), <span class="kw">signif</span>(fit<span class="op">$</span>AIC[<span class="dv">2</span>],<span class="dv">5</span>), <span class="st">&quot;-&quot;</span>) 
}
minaic=<span class="kw">min</span>(aic<span class="op">$</span>aic)
aic<span class="op">$</span>rl=<span class="kw">format</span>(<span class="kw">exp</span>((minaic<span class="op">-</span>aic<span class="op">$</span>aic)<span class="op">/</span><span class="dv">2</span>), <span class="dt">digits=</span><span class="dv">4</span>)
aic<span class="op">$</span>isAICmin =<span class="st"> </span><span class="kw">ifelse</span>(aic<span class="op">$</span>aic<span class="op">==</span>minaic,<span class="st">&quot;Yes&quot;</span>,<span class="st">&quot;-&quot;</span>)

<span class="kw">kable</span>(aic, <span class="dt">format=</span><span class="st">&#39;html&#39;</span>, <span class="dt">row.names=</span>F, <span class="dt">col.names=</span><span class="kw">c</span>(<span class="st">&quot;Compared models&quot;</span>,<span class="st">&quot;AIC&quot;</span>,<span class="st">&quot;Minimum AIC&quot;</span>,<span class="st">&quot;rL&quot;</span>),<span class="dt">digits=</span><span class="dv">30</span>,<span class="dt">format.args=</span><span class="kw">list</span>(<span class="dt">snsmall=</span><span class="dv">0</span>))  <span class="op">%&gt;%</span><span class="st">  </span><span class="kw">kable_styling</span>( <span class="dt">font_size =</span> <span class="dv">14</span>)</code></pre>
<details>
<summary> <em>Show result</em></summary>
<table class="table" style="font-size: 14px; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Compared models
</th>
<th style="text-align:right;">
AIC
</th>
<th style="text-align:left;">
Minimum AIC
</th>
<th style="text-align:left;">
rL
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
1 variable
</td>
<td style="text-align:right;">
317.61
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:left;">
3.841e-08
</td>
</tr>
<tr>
<td style="text-align:left;">
2 variables
</td>
<td style="text-align:right;">
301.80
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:left;">
1.041e-04
</td>
</tr>
<tr>
<td style="text-align:left;">
3 variables
</td>
<td style="text-align:right;">
283.46
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
1.000e+00
</td>
</tr>
<tr>
<td style="text-align:left;">
4 variables
</td>
<td style="text-align:right;">
285.38
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:left;">
3.829e-01
</td>
</tr>
<tr>
<td style="text-align:left;">
5 variables
</td>
<td style="text-align:right;">
286.46
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:left;">
2.231e-01
</td>
</tr>
<tr>
<td style="text-align:left;">
6 variables
</td>
<td style="text-align:right;">
287.66
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:left;">
1.225e-01
</td>
</tr>
<tr>
<td style="text-align:left;">
7 variables
</td>
<td style="text-align:right;">
288.70
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:left;">
7.280e-02
</td>
</tr>
<tr>
<td style="text-align:left;">
8 variables
</td>
<td style="text-align:right;">
290.61
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:left;">
2.802e-02
</td>
</tr>
<tr>
<td style="text-align:left;">
9 variables
</td>
<td style="text-align:right;">
292.61
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:left;">
1.031e-02
</td>
</tr>
<tr>
<td style="text-align:left;">
10 variables
</td>
<td style="text-align:right;">
293.10
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:left;">
8.067e-03
</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<hr />
</details>
<ul>
<li>Try to plot the <span class="math inline">\(AIC\)</span> and the <span class="math inline">\(reL\)</span> with the different models on the <span class="math inline">\(X\)</span>-axis</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(stats)

<span class="co"># plot AIC of all models</span>
<span class="kw">plot</span>(aic<span class="op">$</span>aic, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">1</span>,P), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="kw">floor</span>(<span class="kw">min</span>(aic<span class="op">$</span>aic)),<span class="kw">ceiling</span>(<span class="kw">max</span>(aic<span class="op">$</span>aic))),<span class="dt">ylab=</span><span class="st">&quot;AIC&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;model #&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)

<span class="co"># plot relL of all models</span>
<span class="kw">plot</span>(aic<span class="op">$</span>rl, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">1</span>,P), <span class="dt">ylab=</span><span class="st">&quot;relL&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;model #&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</code></pre>
<details>
<p><summary> <em>Show result</em></summary>
<img src="_main_files/figure-html/unnamed-chunk-26-1.svg" width="672" style="display: block; margin: auto auto auto 0;" /><img src="_main_files/figure-html/unnamed-chunk-26-2.svg" width="672" style="display: block; margin: auto auto auto 0;" /></p>
<hr />
</details>
<div id="think-about-3" class="section level4">
<h4><span class="header-section-number">6.2.1.1</span> Think about:</h4>
<ul>
<li>Which is the best model? Is this correct compared to our <em>oracle knowledge</em>?</li>
<li>How good is it compared to the others?</li>
<li>Can you see a drawback in our model testing approach above? If so, how can we solve that?</li>
</ul>
<details>
<summary> Some possible answers </summary>
<h4>
Some possible answers
</h4>
<ul>
<li>We see that the best model is the one with the 3 first X-variables (in line with our <em>oracle knowledge</em>) and that the second best model (with the first 4 X-variabels) is <span class="math inline">\(\approx 60\%\)</span> as good.</li>
</ul>
<details>
<p><summary> Extra Reading </summary></p>
<ul>
<li>Sometimes it is desirable to compute a significance for rejecting a model in favour of another model. A NULL distribution for the <span class="math inline">\(relL\)</span> statistic is usally obtained through simulation, e.g., using parameteric bootstrapping.</li>
</ul>
<hr />
</details>
<ul>
<li>Now, I this case we happened to know that the first 3 variables was the right one, so the order we choose to include them was correct. However, in the general case, we do not know this. How solve this?
<ul>
<li>Best subset method; involves testing all possible subsets, which is computationally time-consuming and sometimes unfeasible</li>
<li>Lasso</li>
</ul></li>
</ul>
<hr />
</details>
</div>
</div>
</div>
<div id="regularization-lasso-and-feature-selection" class="section level2">
<h2><span class="header-section-number">6.3</span> Regularization | <code>LASSO and Feature selection</code></h2>
<details>
<p><summary> Lecture notes </summary></p>
<p>LASSO stands for Least absolute shrinkage and selection operator (“shrinkage” is another common term for regularization) and is a method for selecting variables to include in a multivariate model.</p>
<p>Classical LASSO builds on RSS of a linear regression model <span class="math inline">\(Y \sim X{\beta}\)</span> with regularization</p>
<details>
<p><summary> Extra Reading </summary></p>
<p>Extensions to glms exists, but then using a regularized likelihood expression</p>
<hr />
</details>
<p>The regularization term <span class="math inline">\(f(\beta) = \lambda\sum_{\beta_i\in\beta} |\beta_i-0|= \lambda\sum_{\beta_i\in\beta} |\beta_i|\)</span></p>
<p>The <span class="math inline">\(\lambda\)</span> parameter sets a limit on the estimation of <span class="math inline">\(\beta\)</span>.</p>
<p>Lasso is traditionally described as RSS with an auxiliary criterion/constraint:</p>
<p><span class="math display">\[min_{{\beta}}\left\{RSS\right\} + \lambda\sum_{\beta_i\in\beta} |\beta_i|.\]</span>
Lasso can also be viewed as a un-normalized Bayesian posterior probability, with a LaPlacean prior on <span class="math inline">\(\beta\)</span>: <span class="math inline">\(\beta_j ∼ LaPlace(0, 1/\lambda)\)</span></p>
<details>
<p><summary> Extra Reading </summary>
Often the regularization term is expressed in terms of the <span class="math inline">\(\ell_1-norm\)</span>, which can be viewed simply a short-hand notation, e.g., the <span class="math inline">\(\ell_1-norm\)</span> of <span class="math inline">\(\beta\)</span> is
<span class="math display">\[ ||\beta||_1 = \sum_{\beta_i\in\beta} |\beta_i|\]</span></p>
<p>There is also a <span class="math inline">\(\ell_2-norm\)</span>:
<span class="math display">\[ ||\beta||_2 = \sqrt{\sum_{\beta_i\in{\beta}} \beta_i^2}\]</span>
which is used, e.g., in ridge regression. This correspond to a Normal prior for Baysians.</p>
<p>We note, BTW, that you might already have encountered an <span class="math inline">\(\ell_2-norm\)</span>: since <span class="math inline">\(RSS = ||Y-X\beta||_2^2\)</span> is simply the square of the <span class="math inline">\(\ell_2\)</span> norm of the residuals.</p>
<ul>
<li>You might also see the notation <span class="math display">\[min_{{\beta}}\left\{RSS\right\} \textrm{ subject to } ||{\beta}||_1 &lt;= t\]</span>
where <span class="math inline">\(t\)</span> is related to <span class="math inline">\(\lambda\)</span>.</li>
</ul>
<hr />
</details>
<p>The optimal values of <span class="math inline">\(\beta\)</span> for different values of <span class="math inline">\(\lambda\)</span> are then estimated, using some algorithm (lars or coordinate descent). A convenient way to think about this is that at very high <span class="math inline">\(\lambda\)</span> values, all <span class="math inline">\(\beta_s\)</span> are 0; by sequentially lowering <span class="math inline">\(\lambda\)</span> more and more <span class="math inline">\(\beta_i\)</span> become non-zero, the most important variables <span class="math inline">\(i\)</span> are included first.</p>
<details>
<p><summary> Extra Reading </summary></p>
<p>The <em>Coordinate descent</em> algorithm is used in the R package <code>glmnet</code>:</p>
<ol style="list-style-type: decimal">
<li>Over a grid of <span class="math inline">\(\lambda\in [0, \infty]\)</span>, do
<ol style="list-style-type: decimal">
<li>Start with all <span class="math inline">\(\beta=0\)</span></li>
<li>until convergence repeat for each <span class="math inline">\(\beta_i\)</span>
<ol style="list-style-type: decimal">
<li>while keeping all other <span class="math inline">\(\beta\)</span> fixed and <span class="math inline">\(\beta_i=0\)</span>, compute partial residuals</li>
<li>estimate <span class="math inline">\(\beta_i\)</span> by RSS on the partial residuals</li>
<li>update $_i using the RSS estimate and <span class="math inline">\(\lambda\)</span>.</li>
</ol></li>
</ol></li>
</ol>
<hr />
</details>
<details>
<p><summary> Extra Reading </summary></p>
<p>Alternatives to LASSO, differing mainly in the auxiliary criterion</p>
<ul>
<li><em>Ridge regression</em> which uses a <span class="math inline">\(\ell_2\)</span> norm</li>
<li><em>Elastic-net</em>, which uses a mixed model combination of the <span class="math inline">\(\ell_1\)</span> norm and the <span class="math inline">\(\ell_2\)</span> norm.</li>
</ul>
<hr />
</details>
</details>
<div id="task-lasso-using-the-glmnet-r-package" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Task | <code>Lasso using the glmnet R-package</code></h3>
<ul>
<li>Use function <code>glmnet</code> to perform LASSO analysis on our example data; relevant arguments of the function include:
<ul>
<li>linear regression (<code>family='gaussian'</code> = default)</li>
<li>LASSO (<code>alpha=1</code> = default)</li>
<li>standardization
<ul>
<li>The variables Y and X must be centered and standardized to ensure that all variables are given equal weight in the model selection.</li>
<li>standardization of <span class="math inline">\(X\)</span> to unit variance in <code>glmnet</code> is obtained by setting the argument <code>standardize=TRUE</code> which is the default</li>
<li>the values of <span class="math inline">\(Y\)</span> is always standardized (?) for <code>family=gaussian</code> (LASSO)</li>
<li>and the coefficients are back-standardized before reported</li>
</ul></li>
</ul></li>
</ul>
<details>
<p><summary> Extra Reading </summary></p>
<p>Standardization in <code>glmnet</code>:</p>
<ul>
<li><span class="math inline">\(x&#39; = \frac{x-\bar{x}}{s/\sqrt{N}}\)</span>
where <span class="math inline">\(\frac{s}{\sqrt{N}}\)</span> is the estimate of the standard deviation of <span class="math inline">\(x\)</span> (and, incidently, can be written using the <span class="math inline">\(\ell_2-\)</span>norm: <span class="math inline">\(\frac{s}{\sqrt{N}} = \frac{\sum_{x_i \in x} (x-\bar{x})^2}{\sqrt{N}} = \frac{||X-\bar{x}||_2}{\sqrt{N}}\)</span>).</li>
</ul>
<hr />
</details>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(glmnet)
<span class="co"># run lasso (alpha=1) for linear model (family=gaussian)</span>
fit =<span class="st"> </span><span class="kw">glmnet</span>(X,Y, <span class="dt">family=</span><span class="st">&quot;gaussian&quot;</span>, <span class="dt">alpha=</span><span class="dv">1</span>, <span class="dt">standardize=</span>T)</code></pre>
<ul>
<li>A graphical way to view the result is to <code>plot</code> the paths of <span class="math inline">\(\beta\)</span> for increasing vaules of <span class="math inline">\(\lambda\)</span>.</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>,<span class="dt">label=</span>T)</code></pre>
<details>
<p><summary> <em>Show result</em></summary>
<img src="_main_files/figure-html/unnamed-chunk-29-1.svg" width="672" style="display: block; margin: auto auto auto 0;" /></p>
<hr />
</details>
<div id="think-about-4" class="section level4">
<h4><span class="header-section-number">6.3.1.1</span> Think about</h4>
<ul>
<li>In which order are variables included (i.e., their <span class="math inline">\(\beta\)</span> becomes non-zero)?</li>
<li>In which direction is the effect</li>
<li>Which lambda should we select?
<ul>
<li>Given our <em>oracle knowledge</em>, where would an appropriate <span class="math inline">\(\lambda\)</span> be?</li>
<li>Can we use that?</li>
</ul></li>
</ul>
<details>
<p><summary> Some possible answers </summary></p>
<h4>
Some possible answers
</h4>
<ul>
<li>The order in the above plot appears to be <span class="math inline">\((1,2,3,7,6,5,10,9,4,8)\)</span> (may vary depending on the simulation)</li>
<li><span class="math inline">\(\beta_i &gt; 0, i\in \{1,2,3,4,7,9\}\)</span>, while <span class="math inline">\(\beta_i&lt;0, i\in \{5,6,8,10\}\)</span></li>
<li>Given <em>oracle knowledge</em>, the correct <span class="math inline">\(\lambda\)</span> appears lie somewhere in the interval <span class="math inline">\([\approx \exp(-0.9), \approx\exp(-2.25)]\)</span></li>
<li>In the typical analysis case, we never have <em>oracle knowledge</em>.</li>
</ul>
<hr />
</details>
</div>
</div>
</div>
<div id="cross-validation" class="section level2">
<h2><span class="header-section-number">6.4</span> Cross-validation</h2>
<details>
<p><summary> Lecture notes </summary></p>
<p>The LASSO model will be different depending on how we set <span class="math inline">\(\lambda\)</span>. A problem is to decide the optimal <span class="math inline">\(\lambda\)</span> to use.</p>
<ul>
<li><span class="math inline">\(\lambda\)</span> too <em>high</em>: risk of missing relevant variables</li>
<li><span class="math inline">\(\lambda\)</span> too <em>low</em>: risk of overfitting</li>
</ul>
<p><code>glmnet</code> addresses this using <em><span class="math inline">\(k\)</span>-fold cross-validation</em> – what is that?</p>
</details>
<div id="cross-validation-how-to-test-for-overfitting" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Cross-validation | <code>How to test for overfitting</code></h3>
<details>
<p><summary> Lecture notes </summary></p>
<p>The ultimate way of testing an estimated model (with parameters) is to apply it to new data and evaluate how well it performs, e.g., by measuring the <em>mean squared error</em>, <span class="math inline">\(MSE\)</span> (<span class="math inline">\(=RSS/N\)</span>).
Naturally, we want to minimize <span class="math inline">\(MSE\)</span>, i.e., the error of the model. In our LASSO application, this means that we want to select the <span class="math inline">\(\lambda\)</span> that minimizes the <span class="math inline">\(MSE\)</span></p>
<p>In cross validation, this approach is emulated by partioning the data at hand into a <em>training</em> and a <em>validation</em> data set. The model parameters are estimated (‘trained’) on the the training data and the validated on the test data.</p>
<p>By chance, this may fail if the partitioning is ‘non-representative’. A solution is to repeat the cross-validation procedure with another partioning.</p>
<p>In <span class="math inline">\(k\)</span>-fold cross validation, the original data is split into <span class="math inline">\(k\)</span> sub-datasets <span class="math inline">\(\{D_1,D_2,\ldots, D_k\}\)</span>.
For <span class="math inline">\(i \in \{1,2,\ldots, k\}\)</span>, set <span class="math inline">\(D_i\)</span> as the test data set and the union of the other datasets be the training data. Perform cross validation as above.</p>
<p>This gives a distribution of <span class="math inline">\(MSE\)</span> from which we can estimate, e.g., mean and standard deviation.</p>
<details>
<p><summary>Additional reading</summary></p>
<p>This distribution allows us to use more elaborate means to select <span class="math inline">\(\lambda\)</span>. One common suggestion is to use the largest <span class="math inline">\(\lambda\)</span> whose <span class="math inline">\(MSE\)</span> is within 1 standard error from the minimum value (called <code>lambda.1se</code> in <code>glmnet</code>). The motivation argued for this choice is <em>parsimony</em>, in the sense that larger <span class="math inline">\(\lambda\)</span> will include fewer variables (hence it is parsimonious in terms of number of included variables).</p>
<p>Here we will limit ourselves to finding the minimum <span class="math inline">\(\lambda\)</span>, called <code>lambda.min</code> in <code>glmnet</code>, but anyone is free to test if <code>lambda.1se</code> gives a different result.</p>
<hr />
</details>
</details>
</div>
<div id="task-determine-optimal-lassolambdausing-cross-validation" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Task | <code>Determine optimal LASSO</code><span class="math inline">\(\lambda\)</span><code>using cross-validation</code></h3>
<ul>
<li>Use the function <code>cv.glmnet</code> to perform cross validation (same options as for <code>glmnet</code>)</li>
<li><code>plot</code> the cross-validation results</li>
<li>Compare with the plot of estimated <span class="math inline">\(\beta_i\)</span> under different <span class="math inline">\(\lambda\)</span>.</li>
<li>Determine the optimal <span class="math inline">\(\lambda\)</span> (the one with minimal error)</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(glmnet)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))
<span class="co"># run lasso (alpha=1) for linear model (family=gaussian)</span>
cvglm=<span class="kw">cv.glmnet</span>(X,Y, <span class="dt">family=</span><span class="st">&quot;gaussian&quot;</span>, <span class="dt">alpha=</span><span class="dv">1</span>, <span class="dt">standardize=</span>T, <span class="dt">nfolds=</span><span class="dv">100</span>)

<span class="kw">plot</span>(cvglm)
<span class="kw">plot</span>(cvglm<span class="op">$</span>glmnet.fit, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>,<span class="dt">label=</span>T)
minlambda=cvglm<span class="op">$</span>lambda.min</code></pre>
<details>
<p><summary> <em>Show result</em></summary>
<img src="_main_files/figure-html/unnamed-chunk-31-1.svg" width="672" style="display: block; margin: auto auto auto 0;" /><img src="_main_files/figure-html/unnamed-chunk-31-2.svg" width="672" style="display: block; margin: auto auto auto 0;" /></p>
<hr />
</details>
<div id="think-about-5" class="section level4">
<h4><span class="header-section-number">6.4.2.1</span> Think about</h4>
<ul>
<li>Which is the <span class="math inline">\(\lambda\)</span> selected by <code>cv.glmnet</code>?</li>
<li>Does this make sense given our <em>oracle knowledge</em>?</li>
</ul>
<details>
<p><summary> Some possible answers </summary></p>
<h4>
Some possible answers
</h4>
<ul>
<li>Cross-validation-selected optimal <span class="math inline">\(\lambda\)</span> is 0.1064891 (<span class="math inline">\(log \lambda=\)</span>-2.2397124)</li>
<li>Yes, this includes only the <em>oracle knowledge</em> correct variables <span class="math inline">\(X_1, X_2, X_3\)</span></li>
</ul>
<hr />
</details>
</div>
</div>
<div id="task-final-lasso-effect-sizes" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Task| <code>Final LASSO effect sizes</code></h3>
<ul>
<li>Finally print a table with the <span class="math inline">\(\beta\)</span> coefficients (including the intercept, <span class="math inline">\(\beta_0\)</span>) for the optimal model (i.e., at minimum <span class="math inline">\(\lambda\)</span>). (Use function<code>coef</code>).</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Actually the following suffice for output on console</span>
<span class="co">#coef(cvglm, s=&quot;lambda.min&quot;)</span>

<span class="co"># But to get a nice table:</span>
<span class="kw">require</span>(dplyr)      <span class="co"># for nice table</span>
<span class="kw">require</span>(kableExtra) <span class="co">#for nice table</span>

coefglm=<span class="kw">as.data.frame</span>(<span class="kw">as.matrix</span>(<span class="kw">coef</span>(cvglm, <span class="dt">s=</span><span class="st">&quot;lambda.min&quot;</span>)))
coefglm=<span class="kw">cbind</span>(<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>),<span class="kw">c</span>(b0, b, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">7</span>)),coefglm)
<span class="kw">names</span>(coefglm)=<span class="kw">c</span>(<span class="st">&quot;beta&quot;</span>,<span class="st">&quot;value (oracle)&quot;</span>, <span class="kw">paste0</span>(<span class="st">&quot;estimate(lambda=&quot;</span>,<span class="kw">signif</span>(minlambda,<span class="dv">3</span>),<span class="st">&quot;)&quot;</span>))
<span class="kw">kable</span>(coefglm, <span class="dt">row.names=</span>F) <span class="op">%&gt;%</span><span class="st">   </span><span class="kw">kable_styling</span>( <span class="dt">font_size =</span> <span class="dv">14</span>)</code></pre>
<details>
<p><summary> <em>Show result</em></summary></p>
<table class="table" style="font-size: 14px; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
beta
</th>
<th style="text-align:right;">
value (oracle)
</th>
<th style="text-align:right;">
estimate(lambda=0.106)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3.0000000
</td>
<td style="text-align:right;">
3.6046135
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.8125118
</td>
<td style="text-align:right;">
0.5905522
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.6009469
</td>
<td style="text-align:right;">
0.4631531
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.7232911
</td>
<td style="text-align:right;">
0.5204561
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.0000000
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.0000000
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.0000000
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.0000000
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.0000000
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.0000000
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
0.0000000
</td>
</tr>
</tbody>
</table>
<hr />
</details>
<div id="think-about-6" class="section level4">
<h4><span class="header-section-number">6.4.3.1</span> Think about</h4>
<ul>
<li>Does the effect sizes make sense – if not can you think of why?</li>
</ul>
<details>
<summary> Some possible answers </summary>
<h4>
Some possible answers
</h4>
<ul>
<li>Well…yes… sort of!
<ul>
<li><span class="math inline">\(\beta_i\)</span> is non-zero only for <em>oracle</em>-known variables <span class="math inline">\(X_1, X_2, X_3\)</span></li>
<li><em>however</em>,they don’t exactly equate our <em>oracle knowledge</em> parameter values – they appear to be scaled.</li>
<li><em>but</em> their relative order of amplitude is right.</li>
</ul></li>
<li>Perhaps the normalization affected scaling.</li>
</ul>
<!-- --------------------- Do not edit this and below ---------------------- -->
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="overfitting.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="session-info-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
